{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-kenya",
   "metadata": {},
   "source": [
    "## GPT2 trained on a Dialogue corpus\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/microsoft/DialoGPT-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "contained-convenience",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\n",
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "realistic-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hey I'm Sinan how are you?\n",
      "DialoGPT: I'm good, you?\n",
      ">> User:What's my name again?\n",
      "DialoGPT: Sinan, you?\n",
      ">> User:What is your name?\n",
      "DialoGPT: Sinan, you?\n",
      ">> User:What a coincidence!\n",
      "DialoGPT: I'm Sinan, you?\n",
      ">> User:Sinan\n",
      "DialoGPT: I'm Sinan, you?\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "auburn-minutes",
   "metadata": {},
   "source": [
    "## Turkish GPT2\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/redrussianarmy/gpt2-turkish-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wanted-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba benim adım Sinan ve ben size anlatacağım hikayemi yazmak istiyorum. Size anlatacağım olay tamamen gerçektir. Uzun boylu iri yapılı 1.60 boy ve 63 kilo zayıf bir bayanım ve sizlere hikayeme anlatacağım olay sizlerin aklinizı da değil sizlerin akliniz da.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import pipeline  \n",
    "\n",
    "\n",
    "turkish_tokenizer = AutoTokenizer.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "\n",
    "turkish_model = AutoModelWithLMHead.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "\n",
    "\n",
    "\n",
    "turkish_generator = pipeline(\n",
    "    'text-generation', model=turkish_model, tokenizer=turkish_tokenizer\n",
    ")\n",
    "\n",
    "print(turkish_generator('Merhaba benim adım Sinan ve ben')[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-collective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "streaming-partition",
   "metadata": {},
   "source": [
    "## Python code completion\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/Sentdex/GPyT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Sentdex/GPyT\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"Sentdex/GPyT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "authentic-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv('data/data/data\n"
     ]
    }
   ],
   "source": [
    "input_code = \"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd\"\"\"  # I'd expect a read_csv here\n",
    "\n",
    "converted = input_code.replace(\"\\n\", \"<N>\")\n",
    "tokenized = tokenizer.encode(converted, return_tensors='pt')\n",
    "resp = model.generate(tokenized, beams=3, max_length=tokenized.shape[1] + 10)\n",
    "\n",
    "decoded = tokenizer.decode(resp[0])\n",
    "reformatted = decoded.replace(\"<N>\",\"\\n\")\n",
    "\n",
    "print(reformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
