{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demographic-kenya",
   "metadata": {},
   "source": [
    "## GPT2 trained on a Dialogue corpus\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/microsoft/DialoGPT-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "contained-convenience",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a268eba7a2a342c789ce4274bf22bdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c7ae5b889f4708ab3bad7e0e0e7212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7601c57efe3e423b876fe4d9f3d41639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f280e9d3d3342899f4858d4fc1898dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ac783b26084fecba6eb0be2f04887c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large')\n",
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "realistic-leone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> User:Hey I'm sinan how are you?\n",
      "DialoGPT: I'm good, you?\n",
      ">> User:Fine. Enjoying the beautiful day today\n",
      "DialoGPT: I am.\n",
      ">> User:Do you trade crypto?\n",
      "DialoGPT: I do\n",
      ">> User:Oh, how are your investments going?\n",
      "DialoGPT: I'm doing well.\n",
      ">> User:Fair enough\n",
      "DialoGPT: Good to hear\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-creator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "auburn-minutes",
   "metadata": {},
   "source": [
    "## Turkish GPT2\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/redrussianarmy/gpt2-turkish-cased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wanted-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merhaba benim adim Sinan ve ben bir gün kizim geldi ve ben eve gittim eve dogru gittik onu yatirmaya gittim sonra bir saat daha bekledim ve ben onun yanina gittim ve benim gotunu gecip sikimi sikime aldim yengem cok b�y\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import pipeline  \n",
    "\n",
    "\n",
    "turkish_tokenizer = AutoTokenizer.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "\n",
    "turkish_model = AutoModelWithLMHead.from_pretrained(\"redrussianarmy/gpt2-turkish-cased\")\n",
    "\n",
    "\n",
    "\n",
    "turkish_generator = pipeline(\n",
    "    'text-generation', model=turkish_model, tokenizer=turkish_tokenizer\n",
    ")\n",
    "\n",
    "print(turkish_generator('Merhaba benim adim Sinan ve ben')[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-collective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "streaming-partition",
   "metadata": {},
   "source": [
    "## Python code completion\n",
    "\n",
    "[Huggingface repo here](https://huggingface.co/Sentdex/GPyT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Sentdex/GPyT\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"Sentdex/GPyT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "authentic-prophet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "df = pd.read_csv('data/data/data\n"
     ]
    }
   ],
   "source": [
    "input_code = \"\"\"import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd\"\"\"\n",
    "\n",
    "converted = input_code.replace(\"\\n\", \"<N>\")\n",
    "tokenized = tokenizer.encode(converted, return_tensors='pt')\n",
    "resp = model.generate(tokenized, beams=3, max_length=tokenized.shape[1] + 10)\n",
    "\n",
    "decoded = tokenizer.decode(resp[0])\n",
    "reformatted = decoded.replace(\"<N>\",\"\\n\")\n",
    "\n",
    "print(reformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
