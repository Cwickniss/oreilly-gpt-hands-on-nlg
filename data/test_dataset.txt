ers = pd.read_csv(url)
# remove rows with missing values
hitters.dropna(inplace=True)
# encode categorical variables as integers
hitters['League'] = pd.factorize(hitters.League)[0]
hitters['Division'] = pd.factorize(hitters.Division)[0]
hitters['NewLeague'] = pd.factorize(hitters.NewLeague)[0]
# define features: exclude career statistics (which start with "C") and the
response (Salary)
feature_cols = [h for h in hitters.columns if h[0] != 'C' and h !=
'Salary']

# define X and y
X = hitters[feature_cols]
y = hitters.Salary

Let's try and predict the salary first using a single decision tree, as illustrated:
from sklearn.tree import DecisionTreeRegressor
# list of values to try for max_depth
max_depth_range = range(1, 21)
# list to store the average RMSE for each value of max_depth
RMSE_scores = []
# use 10-fold cross-validation with each value of max_depth
from sklearn.cross_validation import cross_val_score
for depth in max_depth_range:
treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)
MSE_scores = cross_val_score(treereg, X, y, cv=10,
scoring='mean_squared_error')
RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))
# plot max_depth (x-axis) versus RMSE (y-axis)
plt.plot(max_depth_range, RMSE_scores)

[ 325 ]

Beyond the Essentials

Chapter 12

plt.xlabel('max_depth')
plt.ylabel('RMSE (lower is better)')

RMSE for decision tree models against the max depth of the tree (complexity)

Let's do the same thing, but this time with a random forest:
from sklearn.ensemble import RandomForestRegressor

# list of values to try for n_estimators
estimator_range = range(10, 310, 10)
# list to store the average RMSE for each value of n_estimators
RMSE_scores = []
# use 5-fold cross-validation with each value of n_estimators (WARNING:
SLOW!)
for estimator in estimator_range:
rfreg = RandomForestRegressor(n_estimators=estimator, random_state=1)
MSE_scores = cross_val_score(rfreg, X, y, cv=5,
scoring='mean_squared_error')
RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))

[ 326 ]

Beyond the Essentials

Chapter 12

# plot n_estimators (x-axis) versus RMSE (y-axis)
plt.plot(estimator_range, RMSE_scores)
plt.xlabel('n_estimators')
plt.ylabel('RMSE (lower is better)')

RMSE for random forest models against the max depth of the tree (complexity)

Note already the y-axis; our RMSE is much lower on an average! See how we can obtain a
major increase in predictive power using random forests.

[ 327 ]

Beyond the Essentials

Chapter 12

In random forests, we still have the concept of important features, like we had in decision
trees:
# n_estimators=150 is sufficiently good
rfreg = RandomForestRegressor(n_estimators=150, random_state=1)
rfreg.fit(X, y)
# compute feature importances
pd.DataFrame({'feature':feature_cols,
'importance':rfreg.feature_importances_}).sort('importance', ascending =
False)

So, it looks like the number of years the player has been in the league is still the most
important feature when deciding that player's salary.

[ 328 ]

Beyond the Essentials

Chapter 12

Comparing random forests with decision trees
It is important to realize that just using random forests is not the solution to your data
science problems. While random forests provide many advantages, many disadvantages
also come with them.
The advantages of random forests are as follows:
Their performance is competitive with the best-supervised learning methods
They provide a more reliable estimate of feature importance
They allow you to estimate out-of-sample errors without using train/test splits or
cross-validation
The disadvantages of random forests are as follows:
They are less interpretable (cannot visualize an entire forest of decision trees)
They are slower to train and predict (not great for production or real-time
purposes)

Neural networks
Probably one of the most talked about machine learning models, neural networks are
computational networks built to model animals' nervous systems. Before getting too deep
into the structure, let's take a look at the big advantages of neural networks.
The key component of a neural network is that it is not only a complex structure, but it is
also a complex and flexible structure. This means the following two things:
Neural networks are able to estimate any function shape (this is called being nonparametric)
Neural networks can adapt and literally change their own internal structure
based on their environment

[ 329 ]

Beyond the Essentials

Chapter 12

Basic structure
Neural networks are made up of interconnected nodes (perceptrons) that each take in input
(quantitative value), and output other quantitative values. Signals travel through the
network and eventually end up at a prediction node:

Visualization of neural network interconnected nodes

Another huge advantage of neural networks is that they can be used for supervised
learning, unsupervised learning, and reinforcement learning problems. The ability to be so
flexible, predict many functional shapes, and adapt to their surroundings make neural
networks highly preferable in select fields, as follows:
Pattern recognition: This is probably the most common application of neural
networks. Some examples are handwriting recognition and image processing
(facial recognition).
Entity movement: Examples for this include self-driving cars, robotic animals,
and drone movement.
Anomaly detection: As neural networks are good at recognizing patterns, they
can also be used to recognize when a data point does not fit a pattern. Think of a
neural network monitoring a stock price movement; after a while of learning the
general pattern of a stock price, the network can alert you when something is
unusual in the movement.

[ 330 ]

Beyond the Essentials

Chapter 12

The simplest form of a neural network is a single perceptron. A perceptron, visualized as
follows, takes in some input and outputs a signal:

This signal is obtained by combining the input with several weights and then is put
through some activation function. In cases of simple binary outputs, we generally use the
logistic function, as shown:

To create a neural network, we need to connect multiple perceptrons to each other in a
network fashion, as illustrated in the following graph.

[ 331 ]

Beyond the Essentials

Chapter 12

A multilayer perceptron (MLP) is a finite acyclic graph. The nodes are neurons with
logistic activation:

As we train the model, we update the weights (which are random at first) of the model in
order to get the best predictions possible. If an observation goes through the model and is
outputted as false when it should have been true, the logistic functions in the single
perceptrons are changed slightly. This is called back-propagation. Neural networks are
usually trained in batches, which means that the network is given several training data
points at once several times, and each time, the back-propagation algorithm will trigger an
internal weight change in the network.
It isn't hard to see that we can grow the network very deep and have many hidden layers,
which are associated with the complexity of the neural network. When we grow our neural
networks very deep, we are dipping our toes into the idea of deep learning. The main
advantage of deep neural networks (networks with many layers) is that they can
approximate almost any shape function and they can (theoretically) learn optimal
combinations of features for us and use these combinations to obtain the best predictive
power.

[ 332 ]

Beyond the Essentials

Chapter 12

Let's see this in action. I will be using a module called PyBrain to make my neural
networks. However, first let's take a look at a new dataset, which is a dataset of
handwritten digits. We will first try to recognize digits using a random forest, as shown:
from sklearn.cross_validation import cross_val_score
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
%matplotlib inline
digits = datasets.load_digits()
plt.imshow(digits.images[100], cmap=plt.cm.gray_r, interpolation='nearest')
# a 4 digit

X, y = digits.data, digits.target
# 64 pixels per image
X[0].shape
# Try Random Forest
rfclf = RandomForestClassifier(n_estimators=100, random_state=1)
cross_val_score(rfclf, X, y, cv=5, scoring='accuracy').mean()
0.9382782

[ 333 ]

Beyond the Essentials

Chapter 12

Pretty good! An accuracy of 94% is nothing to laugh at, but can we do even better?
Warning! The PyBrain syntax can be a bit tricky.

from
from
from
from
from
from

pybrain.datasets
pybrain.utilities
pybrain.tools.shortcuts
pybrain.supervised.trainers
pybrain.structure.modules
numpy import ravel

import
import
import
import
import

ClassificationDataSet
percentError
buildNetwork
BackpropTrainer
SoftmaxLayer

# pybrain has its own data sample class that we must add
# our training and test set to
ds = ClassificationDataSet(64, 1 , nb_classes=10)
for k in xrange(len(X)):
ds.addSample(ravel(X[k]),y[k])
# their equivalent of train test split
test_data, training_data = ds.splitWithProportion( 0.25 )

# pybrain's version of dummy variables
test_data._convertToOneOfMany( )
training_data._convertToOneOfMany( )

print test_data.indim # number of pixels going in
# 64
print test_data.outdim # number of possible options (10 digits)
# 10

# instantiate the model with 64 hidden layers (standard params)
fnn = buildNetwork( training_data.indim, 64, training_data.outdim,
outclass=SoftmaxLayer )
trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1,
learningrate=0.01 , verbose=True, weightdecay=0.01)
# change the number of epochs to try to get better results!
trainer.trainEpochs (10) # 10 batches

[ 334 ]

Beyond the Essentials

Chapter 12

print 'Percent Error on Test dataset: ' , \
percentError( trainer.testOnClassData (
dataset=test_data )
, test_data['class'] )

The model will output a final error on a test set:
Percent Error on Test dataset: 4.67706013363
accuracy = 1 - .0467706013363
accuracy
0.95322

Already better! Both the random forests and neural networks do very well with this
problem because both of them are non-parametric, which means that they do not rely on
the underlying shape of the data to make predictions. They are able to estimate any shape
of function.
To predict the shape, we can use the following code:
plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')

fnn.activate(X[0])
array([ 0.92183643,
0.00718017,

0.00126609,
0.00825521,

0.00303146,
0.00917995,

[ 335 ]

0.00387049,
0.00696929,

0.01067609,
0.02773482])

Beyond the Essentials

Chapter 12

The array represents a probability for every single digit, which means that there is a 92%
chance that the digit in the preceding screenshot is a 0 (which it is). Note how the next
highest probability is for a 9, which makes sense because 9 and 0 have similar shapes
(ovular).
Neural networks do have a major flaw. If left alone, they have a very high variance. To see
this, let's run the exact same code as the preceding one and train the exact same type of
neural network on the exact same data, as illustrated:
# Do it again and see the difference in error
fnn = buildNetwork( training_data.indim, 64, training_data.outdim,
outclass=SoftmaxLayer )
trainer = BackpropTrainer( fnn, dataset=training_data, momentum=0.1,
learningrate=0.01 , verbose=True, weightdecay=0.01)
# change the number of eopchs to try to get better results!
trainer.trainEpochs (10)
print ('Percent Error on Test dataset: ' , \
percentError( trainer.testOnClassData (
dataset=test_data )
, test_data['class'] ) )
accuracy = 1 - .0645879732739
accuracy
0.93541

See how just rerunning the model and instantiating different weights made the network
turn out to be different than before? This is a symptom of being a high variance model. In
addition, neural networks generally require many training samples in order to combat the
high variances of the model and also require a large amount of computation power to work
well in production environments.

[ 336 ]

Beyond the Essentials

Chapter 12

Summary
This concludes our long journey into the principles of data science. In the last 300-odd
pages, we looked at different techniques in probability, statistics, and machine learning to
answer the most difficult questions out there. I would like to personally congratulate you
on making it through this book. I hope that it proved useful and inspired you to learn even
more!
This isn't everything I need to know?
Nope! There is only so much I can fit into a principles level book. There is still so much to
learn.
Where can I learn more?
I recommend going to find open source data challenges (https://www.kaggle.com/ is a
good source) for this. I'd also recommend seeking out, trying, and solving your own
problems at home!
When do I get to call myself a data scientist?
When you begin cultivating actionable insights from datasets, both large and small, that
companies and people can use, then you have the honor of calling yourself a true data
scientist.
In next chapter, we will apply concepts learned in this book to real-life case studies,
including building predictive models to predict the stock market. We will also cover the
emerging machine learning topic of TensorFlow.

[ 337 ]

13
Case Studies
In this chapter, we will take a look at a few case studies to help you develop a better
understanding of the topics we've seen so far.

Case study 1 – Predicting stock prices
based on social media
Our first case study will be quite exciting! We will attempt to predict the price of the stock
of a publicly traded company using only social media sentiment. While this example will
not use any explicit statistical/machine learning algorithms, we will utilize exploratory data
analysis (EDA) and use visuals in order to achieve our goal.

Text sentiment analysis
When talking about sentiment, it should be clear what is meant. By sentiment, I am
referring to a quantitative value (at the interval level) between -1 and 1. If the sentiment
score of a text piece is close to -1, it is said to have negative sentiment. If the sentiment score
is close to 1, then the text is said to have positive sentiment. If the sentiment score is close to
0, we say it has neutral sentiment. We will use a Python module called TextBlob to
measure our text sentiment:
from textblob import TextBlob
# use the textblob module to make a function called stringToSentiment that
returns a sentences sentiment
def stringToSentiment(text):
return TextBlob(text).sentiment.polarity

Case Studies

Chapter 13

Now, we can use this function, which calls the Textblob module to score text out of the
box:
stringToSentiment('i hate you')
# -0.8
stringToSentiment('i love you')
# 0.5
stringToSentiment('i see you')
# 0.0

Now, let's read in our tweets for our study:
# read in tweets data into a dataframe
from textblob import TextBlob
import pandas as pd
%matplotlib inline
# these tweets are from last May and are about Apple (AAPL)
tweets = pd.read_csv('../data/so_many_tweets.csv')
tweets.head()

Exploratory data analysis
So we have four columns, as follows:
Text: Unstructured text at the nominal level
Date: Datetime (we will think of datetime in a continuous way)
Status: Status unique ID at the nominal level
Retweet: Status ID of tweet showing that this tweet was a retweet at the nominal

level

[ 339 ]

Case Studies

Chapter 13

So we have four columns, but how many rows? Also, what does each row represent? It
seems that each row represents a single tweet about the company:
tweets.shape

The output is as follows:
(52512, 4)

So, we have four columns and 52512 tweets/rows at our disposal! Oh boy… Our goal here
is to eventually use the tweets' sentiments, so we will likely need a sentiment column in the
DataFrame. Using our fairly straightforward function from the previous example, let's add
this column!
# create a new column in tweets called sentiment that maps
stringToSentiment to the text column
tweets['sentiment'] = tweets['Text'].apply(stringToSentiment)
tweets.head()

The preceding code will apply the stringToSentiment function to each and every
element in the Text column of the tweets DataFrame:
tweets.head()

So, now we have a sense for the sentiment score for each tweet in this dataset. Let's simplify
our problem and try to use an entire days' worth of tweets to predict whether or not the
price of AAPL will increase within 24 hours. If this is the case, we have another issue here.
The Date column reveals that we have multiple tweets for each day. Just look at the first
five tweets; they are all on the same day. We will resample this dataset in order to get a
sense of the average sentiment of the stock on Twitter every day.

[ 340 ]

Case Studies

Chapter 13

We will do this in three steps:
1. We will ensure that the Date column is of the Python datetime type.
2. We will replace our DataFrame's index with the datetime column (which allows
us to use complex datetime functions).
3. We will resample the data so that each row, instead of representing a tweet, will
represent a single day with an aggregated sentiment score for each day:
The index of the DataFrame is a special series used to identify rows in our
structure. By default, a DataFrame will use incremental integers to
represent rows (0 for the first row, 1 for the second row, and so on).
import pandas as pd
tweets.index = pd.RangeIndex(start=0, stop=52512, step=1)
# As a list, we can splice it
list(tweets.index)[:5]
[0, 1, 2, 3, 4]

4. Let's tackle this date issue now! We will ensure that the Date column is of the
Python datetime type:
# cast the date column as a datetime
tweets['Date'] = pd.to_datetime(tweets.Date)
tweets['Date'].head()

Date
2015-05-24 03:46:08
2015-05-24 03:46:08
2015-05-24 04:17:42
2015-05-24 04:17:42
2015-05-24 04:13:22
2015-05-24 04:13:22
2015-05-24 04:08:34
2015-05-24 04:08:34
2015-05-24 04:04:42
2015-05-24 04:04:42
Name: Date, dtype: datetime64[ns]

[ 341 ]

Case Studies

Chapter 13

5. We will replace our DataFrame's index with the datetime column (which allows
us to use complex datetime functions):
tweets.index = tweets.Date
tweets.index

Index([u'2015-05-24 03:46:08', u'2015-05-24 04:17:42',
u'2015-05-24 04:13:22',
u'2015-05-24 04:08:34', u'2015-05-24 04:04:42',
u'2015-05-24 04:00:01',
u'2015-05-24 03:54:07', u'2015-05-24 04:25:29',
u'2015-05-24 04:24:47',
u'2015-05-24 04:06:42',
...
u'2015-05-02 16:30:02', u'2015-05-02 16:29:35',
u'2015-05-02 16:28:26',
u'2015-05-02 16:27:53', u'2015-05-02 16:27:02',
u'2015-05-02 16:26:39',
u'2015-05-02 16:25:00', u'2015-05-02 16:23:39',
u'2015-05-02 16:23:38',
u'2015-05-02 16:23:21'],
dtype='object', name=u'Date', length=52512)

tweets.head()

[ 342 ]

Case Studies

Chapter 13

Note that the black index on the left used to be numbers, but now is the
exact datetime that the tweet was sent.

6. Resample the data so that each row, instead of representing a tweet, will
represent a single day with an aggregated sentiment score for each day:
# create a dataframe called daily_tweets which resamples tweets
by D, averaging the columns
daily_tweets = tweets[['sentiment']].resample('D', how='mean')
# I only want the sentiment column in my new Dataframe.
daily_tweets.head()

That's looking better! Now, each row represents a single day and the sentiment score
column is showing us an average sentiment for the day. Let's see how many days' worth of
tweets we have:
daily_tweets.shape
(23, 1)

OK, so we went from over 50,000 tweets to only 23 days! Now, let's take a look at the
progression of sentiment over several days:
# plot the sentiment as a line graph
daily_tweets.sentiment.plot(kind='line')

[ 343 ]

Case Studies

Chapter 13

Average daily sentiment in regard to a speciﬁc company for 23 days in May 2015

import pandas as pd
import pandas_datareader as pdr
import datetime
historical_prices = pdr.get_data_yahoo('AAPL',
start=datetime.datetime(2015, 5, 2),
end=datetime.datetime(2015, 5, 25))

prices = pd.DataFrame(historical_prices)
prices.head()

[ 344 ]

Case Studies

Chapter 13

Now, two things are primarily of interest to us here:
We are really only interested in the Close column, which is the final price set for
the trading day.
We also need to set the index of this DataFrame to be datetimes, so that we can
merge the sentiment and the price DataFrames together:
prices.info() #the columns aren't numbers!

<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 15 entries, 2015-05-22 to 2015-05-04
Data columns (total 8 columns):
Adj_Close
15 non-null object
Close
15 non-null object
# NOT A NUMBER
Date
15 non-null object
High
15 non-null object
Low
15 non-null object
Open
15 non-null object
Symbol
15 non-null object
Volume
15 non-null object
dtypes: object(8)

Let's fix that. While we're at it, let's also fix Volume, which represents the number of stocks
traded on that day:
# cast the column as numbers
prices.Close= not_null_close.Close.astype('float')
prices.Volume = not_null_close.Volume.astype('float')

Now, let's try to plot both the volume and price of AAPL in the same graph:
# plot both volume and close as line graphs in the same graph, what do you
notice is the problem?
prices[["Volume", 'Close']].plot()

[ 345 ]

Case Studies

Chapter 13

Trade volume versus date

Woah, what's wrong here? Well, if we look carefully, Volume and Close are on very
different scales!
prices[["Volume", 'Close']].describe()

[ 346 ]

Case Studies

Chapter 13

And by a lot! The Volume column has a mean in the tens of millions, while the average
closing price is merely 125!
from sklearn.preprocessing import StandardScaler
# scale the columns by z scores using StandardScaler
# Then plot the scaled data
s = StandardScaler()
only_prices_and_volumes = prices[["Volume", 'Close']]
price_volume_scaled = s.fit_transform(only_prices_and_volumes)
pd.DataFrame(price_volume_scaled, columns=["Volume", 'Close']).plot()

Correlation of volume and closing prices

That looks much better! You can see how, as the price of AAPL went down somewhere in
the middle, the volume of trading also went up! This is actually fairly common:
# concatinate prices.Close, and daily_tweets.sentiment
merged = pd.concat([prices.Close, daily_tweets.sentiment], axis=1)
merged.head()

[ 347 ]

Case Studies

Chapter 13

Hmm, why are there some null Close values? Well, if you look up May 2, 2015 on a
calendar, you will see that it is a Saturday and the markets are closed on Saturdays,
meaning there cannot be a closing price! So, we need to make a decision on whether or not
to remove these rows because we still have sentiment for that day. Eventually, we will be
attempting to predict the next day's closing price and whether the price increased or not, so
let's go ahead and remove any null values in our dataset:
# Delete any rows with missing values in any column
merged.dropna(inplace=True)

Now, let's attempt to graph our plot:
merged.plot()
# wow that looks awful

Closing price/sentiment score versus date

[ 348 ]

Case Studies

Chapter 13

Wow, that's terrible. Once again, we must scale our features in order to gain any valuable
insight:
# scale the columns by z scores using StandardScaler
from sklearn.preprocessing import StandardScaler
s = StandardScaler()
merged_scaled = s.fit_transform(merged)
pd.DataFrame(merged_scaled, columns=merged.columns).plot()
# notice how sentiment seems to follow the closing price

Closing price versus sentiment score

Much better! You can start to see how the closing price of the stock actually does seem to
move with our sentiment. Let's take this one step further and attempt to apply a supervised
learning model. For this to work, we need to define our features and our response. Recall
that our response is the value that we wish to predict, and our features are values that we
will use to predict the response.
If we look at each row of our data, we have a sentiment and closing price for that day.
However, we wish to use today's sentiment to predict tomorrow's stock price and whether
it increased or not. Think about it; it would be kind of cheating because today's sentiment
will include tweets from after the closing price was finalized. To simplify this, we will
ignore any tweet as a feature for the prediction of today's price.

[ 349 ]

Case Studies

Chapter 13

So, for each row, our response should be today's closing price, while our feature should be
yesterday's sentiment of the stock. To do this, I will use a built-in function in Pandas called
shift to shift our sentiment column one item backward:
# Shift the sentiment column backwards one item
merged['yesterday_sentiment'] = merged['sentiment'].shift(1)
merged.head()

Dataframe with yesterday's sentiment included

Ah good, so now, for each day we have our true feature, which is yesterday_sentiment.
Note that in our heads (first five rows), we have a new null value! This is because, on the
first day, we don't have a value from yesterday, so we will have to remove it. But before we
do, let's define our response column.
We have two options:
Keep our response quantitative and use a regression analysis
Convert our response to a qualitative state and use classification
Which route to choose is up to the data scientist and depends on the situation. If you
merely wish to associate sentiment with a movement in price, then I recommend using the
classification route. If you wish to associate sentiment with the amount of movement, I
recommend a regression. I will do both!

[ 350 ]

Case Studies

Chapter 13

Regression route
We are already good to go on this front! We have our response and our single feature. We
will first have to remove that one null value before continuing:
# Make a new dataframe for our regression and drop the null values
regression_df = merged[['yesterday_sentiment', 'Close']]
regression_df.dropna(inplace=True)
regression_df.head()

Let's use both a random forest and a linear regression and see which performs better, using
root-mean-square error (RMSE) as our metric:
# Imports for our regression
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import cross_val_score
import numpy as np

We will use a cross-validated RMSE in order to compare our two models:
# Our RMSE as a result of cross validation linear regression
linreg = LinearRegression()
rmse_cv = np.sqrt(abs(cross_val_score(linreg,
regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,
scoring='mean_squared_error').mean()))
rmse_cv
3.49837
# Our RMSE as a result of cross validation random forest
rf = RandomForestRegressor()

[ 351 ]

Case Studies

Chapter 13

rmse_cv = np.sqrt(abs(cross_val_score(rf,
regression_df[['yesterday_sentiment']], regression_df['Close'], cv=3,
scoring='mean_squared_error').mean()))
rmse_cv
3.30603

Look at our RMSE; it's about 3.5 for both models, meaning that on average, our model is off
by about 3.5 dollars, which is actually a big deal considering our stock price likely doesn't
move that much:
regression_df['Close'].describe()
count
mean
std
sign)
min
25%
50%
75%
max

14.000000
128.132858
2.471810

# Our standard deviation is less than our RMSE (bad

125.010002
125.905003
128.195003
130.067505
132.539993

Another way to test the validity of our model is by comparing our RMSE to the null
model's RMSE. The null model for a regression model is predicting the average value for
each value:
# null model for regression
mean_close = regression_df['Close'].mean()
preds = [mean_close]*regression_df.shape[0]
preds
from sklearn.metrics import mean_squared_error
null_rmse = np.sqrt(mean_squared_error(preds, regression_df['Close']))
null_rmse

2.381895

Because our model did not beat the null model, perhaps regression isn't the best way to go.

[ 352 ]

Case Studies

Chapter 13

Classification route
For classification, we have a bit more work to do because we don't have a categorical
response yet. To make one, we need to transform the closing column into some categorical
option. I will choose to make the following response. I will make a new column called
change_close_big_deal, defined as follows:

So, our response will be 1 if our response changed significantly, and 0 if the change in stock
was negligible:
# Imports for our classification
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
import numpy as np

# Make a new dataframe for our classification and drop the null values
classification_df = merged[['yesterday_sentiment', 'Close']]
# variable to represent yesterday's closing price
classification_df['yesterday_close'] = classification_df['Close'].shift(1)

# column that represents the precent change in price since yesterday
classification_df['percent_change_in_price'] = (classification_df['Close']classification_df['yesterday_close']) /
classification_df['yesterday_close']

# drop any null values
classification_df.dropna(inplace=True)
classification_df.head()

# Our new classification response
classification_df['change_close_big_deal'] =
abs(classification_df['percent_change_in_price'] ) > .01
classification_df.head()

[ 353 ]

Case Studies

Chapter 13

Our DataFrame with a new column called change_close_big_deal is either True or
False.
Let's now perform the same cross-validation as we did with our regression, but this time,
we will be using the accuracy feature of our cross-validation module and, instead of a
regression module, we will be using two classification machine learning algorithms:
# Our accuracy as a result of cross validation random forest
rf = RandomForestClassifier()
accuracy_cv = cross_val_score(rf,
classification_df[['yesterday_sentiment']],
classification_df['change_close_big_deal'], cv=3,
scoring='accuracy').mean()
accuracy_cv
0.1777777

Gosh! Not so good, so let's try logistic regression instead:
# Our accuracy as a result of cross validation logistic regression
logreg = LogisticRegression()
accuracy_cv = cross_val_score(logreg,
classification_df[['yesterday_sentiment']],
classification_df['change_close_big_deal'], cv=3,
scoring='accuracy').mean()
accuracy_cv
0.5888

[ 354 ]

Case Studies

Chapter 13

Better! But, of course, we should check it with our null model's accuracy:
# null model for classification
null_accuracy = 1 - classification_df['change_close_big_deal'].mean()
null_accuracy
0.5833333

Whoa, our model can beat the null accuracy, meaning that our machine learning algorithm
can predict the movement of a stock price using social media sentiment better than just
randomly guessing!

Going beyond with this example
There are many ways that we could have enhanced this example to make a more robust
prediction. We could have included more features, including a moving average of
sentiment, instead of looking simply at the previous day's sentiment. We could have also
brought in more examples to enhance our idea of sentiment. We could have looked at
Facebook, the media, and so on, for more information on how we believe the stock will
perform in the future.
We really only had 14 data points, which is far from sufficient to make a production-ready
algorithm. Of course, for the purposes of this book, this is enough, but if we are serious
about making a financial algorithm that can effectively predict stock price movement, we
will have to obtain many more days of media coverage and prices.
We could have spent more time optimizing our parameters in our models by utilizing the
gridsearchCV module in the sklearn package to get the most out of our models. There
are other models that exist that deal specifically with time series data (data that changes
over time), including a model called AutoRegressive Integrated Moving Average
(ARIMA). Models such as ARIMA and similar ones attempt to focus and zero in on specific
time series features.

[ 355 ]

Case Studies

Chapter 13

Case study 2 – Why do some people cheat
on their spouses?
In 1978, a survey was conducted on housewives in order to discern factors that led them to
pursue extramarital affairs. This study became the basis for many future studies of both
men and women, all attempting to focus on features of people and marriages that led either
partner to seek partners elsewhere behind their spouse's back.
Supervised learning is not always about prediction. In this case study, we will purely
attempt to identify a few factors of the many that we believe might be the most important
factors that lead someone to pursue an affair.
First, let's read in the data:
# Using dataset of a 1978 survey conducted to measure likliehood of women
to perform extramarital affairs
# http://statsmodels.sourceforge.net/stable/datasets/generated/fair.html
import statsmodels.api as sm
affairs_df = sm.datasets.fair.load_pandas().data
affairs_df.head()

The statsmodels website provides a data dictionary, as follows:
rate_marriage: The rating given to the marriage (given by the wife); 1 = very

poor, 2 = poor, 3 = fair, 4 = good, 5 = very good; ordinal level
age: Age of the wife; ratio level
yrs_married: Number of years married: ratio level

[ 356 ]

Case Studies

Chapter 13

children: Number of children between husband and wife: ratio level
religious: How religious the wife is; 1 = not, 2 = mildly, 3 = fairly, 4 = strongly;

ordinal level
educ: Level of education; 9 = grade school, 12 = high school, 14 = some college, 16 =
college graduate, 17 = some graduate school, 20 = advanced degree; ratio level
occupation: 1 = student; 2 = farming, agriculture; semi-skilled, or unskilled worker; 3
= white-collar; 4 = teacher, counselor, social worker, nurse; artist, writer; technician,
skilled worker; 5 = managerial, administrative, business; 6 = professional with advanced
degree; nominal level
occupation_husb: Husband's occupation. Same as occupation; nominal level
affairs: Measure of time spent in extramarital affairs; ratio level
Okay, so we have a quantitative response, but my question is simply what factors cause
someone to have an affair. The exact number of minutes or hours does not really matter
that much. For this reason, let's make a new categorical variable called affair_binary,
which is either true (they had an affair for more than 0 minutes) or false (they had an
affair for 0 minutes):
# Create a categorical variable
affairs_df['affair_binary'] = (affairs_df['affairs'] > 0)

Again, this column has either a true or a false value. The value is true if the person had
an extramarital affair for more than 0 minutes. The value is false otherwise. From now on,
let's use this binary response as our primary response. Now, we are trying to find which of
these variables are associated with our response, so let's begin.
Let's start with a simple correlation matrix. Recall that this matrix shows us linear
correlations between our quantitative variables and our response. I will show the
correlation matrix as both a matrix of decimals and also as a heat map. Let's see the
numbers first:
# find linear correlations between variables and affair_binary
affairs_df.corr()

[ 357 ]

Case Studies

Chapter 13

Correlation matrix for extramarital aﬀairs data from a Likert survey conducted in 1978

Remember that we ignore the diagonal series of 1s because they are merely telling us that
every quantitative variable is correlated with itself. Note the other correlated variables,
which are the values closest to 1 and -1 in the last row or column (the matrix is always
symmetrical across the diagonal).
We can see a few standout variables:
affairs
age
yrs_married
children

These are the top four variables with the largest magnitude (absolute value). However, one
of these variables is cheating. The affairs variable is the largest in magnitude, but is
obviously correlated to affair_binary because we made the affair_binary
variable directly based on affairs. So let's ignore that one.

[ 358 ]

Case Studies

Chapter 13

Let's take a look at our correlation heat map to see whether our views can be seen there:
import seaborn as sns
sns.heatmap(affairs_df.corr())

Correlation matrix

The same correlation matrix, but this time as a heat map. Note the colors close to dark red
and dark blue (excluding the diagonal).
We are looking for the dark red and dark blue areas of the heat map. These colors are
associated with the most correlated features.
Remember correlations are not the only way to identify which features are associated with
our response. This method shows us how linearly correlated the variables are with each
other. We may find another variable that affects affairs by evaluating the coefficients of a
decision tree classifier. These methods might reveal new variables that are associated with
our variables, but not in a linear fashion.

[ 359 ]

Case Studies

Chapter 13

Also notice that there are two variables here that don't actually belong…
Can you spot them? These are the occupation and occupation_husb
variables. Recall earlier that we deemed them as nominal and therefore
have no right to be included in this correlation matrix. This is because
Pandas, unknowingly, casts them as integers and now considers them as
quantitative variables. Don't worry, we will fix this soon.
First, let's make ourselves an X and a y DataFrame:
affairs_X = affairs_df.drop(['affairs', 'affair_binary'], axis=1)
# data without the affairs or affair_binary column
affairs_y = affairs_df['affair_binary']

Now, we will instantiate a decision tree classifier and cross-validate our model in order to
determine whether or not the model is doing an okay job at fitting our data:
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
# instantiate the model
from sklearn.cross_validation import cross_val_score
# import our cross validation module
# check the accuracy on the training set
scores = cross_val_score(model, affairs_X, affairs_y, cv=10)
print( scores.mean(), "average accuracy" )
0.659756806845 average accuracy

print( scores.std(), "standard deviation") # very low, meaning variance of
the model is low
0.0204081732291 standard deviation
# Looks ok on the cross validation side

Because our standard deviation is low, we may make the assumption that the variance of
our model is low (because variance is the square of standard deviation). This is good
because that means that our model is not fitting wildly differently on each fold of the cross
validation and that it is generally a reliable model.

[ 360 ]

Case Studies

Chapter 13

Because we agree that our decision tree classifier is generally a reliable model, we can fit the
tree to our entire dataset and use the importance metric to identify which variables our tree
deems the most important:
# Explore individual features that make the biggest impact
# rate_marriage, yrs_married, and occupation_husb. But one of these
variables doesn't quite make sense right?
# Its the occupation variable, because they are nominal, their
interpretations
model.fit(affairs_X, affairs_y)
pd.DataFrame({'feature':affairs_X.columns,
'importance':model.feature_importances_}).sort_values('importance').tail(3)

So, yrs_married and rate_marriage both are important, but the most important
variable is occupation_husb. But that doesn't make sense because that variable is
nominal! So, let's apply our dummy variable technique wherein we create new columns
that represent each option for occupation_husb and also for occupation.
Firstly, for the occupation column:
# Dummy Variables:
# Encoding qualitiative (nominal) data using separate columns (see slides
for linear regression for more)
occuptation_dummies = pd.get_dummies(affairs_df['occupation'],
prefix='occ_').iloc[:, 1:]
# concatenate the dummy variable columns onto the original DataFrame
(axis=0 means rows, axis=1 means columns)
affairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)
affairs_df.head()

[ 361 ]

Case Studies

Chapter 13

This new DataFrame has many new columns:

Remember, these new columns, occ_2.0, occ_4.0, and so on, represent a binary variable
that represents whether or not the wife holds job 2, or 4, and so on:
# Now for the husband's job
occuptation_dummies = pd.get_dummies(affairs_df['occupation_husb'],
prefix='occ_husb_').iloc[:, 1:]
# concatenate the dummy variable columns onto the original DataFrame
(axis=0 means rows, axis=1 means columns)
affairs_df = pd.concat([affairs_df, occuptation_dummies], axis=1)
affairs_df.head()
(6366, 15)

Now we have 15 new columns! Let's run our tree again and find the most important
variables:
# remove appropiate columns for feature set affairs_X =
affairs_df.drop(['affairs', 'affair_binary', 'occupation',
'occupation_husb'], axis=1) affairs_y = affairs_df['affair_binary'] model =
DecisionTreeClassifier() from sklearn.cross_validation import
cross_val_score # check the accuracy on the training set scores =
cross_val_score(model, affairs_X, affairs_y, cv=10) print scores.mean(),
"average accuracy"
print (scores.std(), "standard deviation") # very low, meaning variance of
the model is low # Still looks ok # Explore individual features that make
the biggest impact model.fit(affairs_X, affairs_y)

[ 362 ]

Case Studies

Chapter 13

pd.DataFrame({'feature':affairs_X.columns,
'importance':model.feature_importances_}).sort_values('importance').tail(10
)

And there you have it:
rate_marriage: The rating of the marriage, as told by the decision tree
children: The number of children they had, as told by the decision tree and our

correlation matrix
yrs_married: The number of years they had been married, as told by the
decision tree and our correlation matrix
educ: The level of education the women had, as told by the decision tree
age: The age of the women, as told by the decision tree and our correlation
matrix

These seem to be the top five most important variables in determining whether or not a
woman from the 1978 survey would be involved in an extramarital affair.

Case study 3 – Using TensorFlow
I would like to finish off our time together by looking at a somewhat more modern machine
learning module called TensorFlow.

[ 363 ]

Case Studies

Chapter 13

TensorFlow is an open source machine learning module that is used primarily for its
simplified deep learning and neural network abilities. I would like to take some time to
introduce the module and solve a few quick problems using TensorFlow. The syntax for
TensorFlow (like PyBrain in Chapter 12, Beyond the Essentials) is a bit different than our
normal scikit-learn syntax, so I will be going over it step by step. Let's start with some
imports:
from sklearn import datasets, metrics
import tensorflow as tf
import numpy as np
from sklearn.cross_validation import train_test_split
%matplotlib inline

Our imports from sklearn include train_test_split, datasets, and metrics. We will
be utilizing our train test splits to reduce overfitting, we will use datasets in order to import
our iris classification data, and we'll use the metrics module in order to calculate some
simple metrics for our learning models.
TensorFlow learns in a different way in that it is always trying to minimize an error
function. It does this by iteratively going through our entire dataset and, every so often,
updates our model to better fit the data.
It is important to note that TensorFlow doesn't just implement neural networks, but can
implement even simpler models as well. For example, let's implement a classic logistic
regression using TensorFlow:
# Our data set of iris flowers
iris = datasets.load_iris()
# Load datasets and split them for training and testing
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target)

####### TENSORFLOW #######
# Here is tensorflow's syntax for defining features.
# We must specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]
# notice the dimension is set to four because we have four columns
# We set our "learning rate" which is a decimal that tells the network
# how quickly to learn
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)
# A learning rate closer to 0 means the network will learn slower

[ 364 ]

Case Studies

Chapter 13

# Build a linear classifier (logistic regression)
# note we have to tell tensorflow the number of classes we are looking for
# which are 3 classes of iris
classifier =
tf.contrib.learn.LinearClassifier(feature_columns=feature_columns,
optimizer=optimizer,
n_classes=3)
# Fit model. Uses error optimization techniques like stochastic gradient
descent
classifier.fit(x=X_train,
y=y_train,
steps=1000) # number of iterations

I will point out the key lines of code from the preceding snippet to really solidify what is
happening during training:
1. feature_columns = [tf.contrib.layers.real_valued_column("",
dimension=4)]:
Here, I am creating four input columns that we know correlate to the
flowers' sepal length, sepal width, petal length, and petal width.
2. optimizer =
tf.train.GradientDescentOptimizer(learning_rate=.1):
Here, I am telling TensorFlow to optimize using something called
gradient descent, which means that we will define an error function
(which will happen in the next step) and, little by little, we will work
our way to minimize this error function.
Our learning rate should hover close to 0 because we want our model
to learn slowly. If our model learns too quickly, it might "skip over" the
correct answer!
3. classifier =
tf.contrib.learn.LinearClassifier(feature_columns=feature_colum
ns, optimizer=optimizer, n_classes=3):
When we specify LinearClassifier, we are denoting the same error
function that logistic regression is minimizing, meaning that this
classifier is attempting to work as a logistic regression classifier.
We give the model our feature_columns as defined in Step 1.
The optimizer is the method of minimizing our error function; in this
case, we chose gradient descent.
We must also specify our number of classes as being 3. We know that
we have three different iris flowers that the model could choose from.

[ 365 ]

Case Studies

Chapter 13

4. classifier.fit(x=X_train, y=y_train, steps=1000):
The training looks similar to a scikit-learn model, with an added
parameter called steps. Steps tell us how many times we would like
to go over our dataset. So, when we specify 1000, we are iterating over
our dataset. The more steps we take, the more the model gets a chance
to learn.
Phew! When we run the preceding code, a linear classifier (logistic regression) model is
being fit, and when it is done, it is ready to be tested:
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
y=y_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.973684

Excellent! It is worth noting that when using TensorFlow, we may also utilize a similarly
simple predict function:
# Classify two new flower samples.
new_samples = np.array(
[[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)
y = classifier.predict(new_samples)
print('Predictions: {}'.format(str(y)))
Predictions: [1 2]

Now, let's compare this with a standard scikit-learn logistic regression to see who won:
from sklearn.linear_model import LogisticRegression
# compare our result above to a simple scikit-learn logistic regression
logreg = LogisticRegression()
# instantiate the model
logreg.fit(X_train, y_train)
# fit it to our training set
y_predicted = logreg.predict(X_test)
# predict on our test set, to avoid overfitting!

accuracy = metrics.accuracy_score(y_predicted, y_test)
# get our accuracy score

[ 366 ]

Case Studies

Chapter 13

accuracy
# It's the same thing!

Wow, so it seems that with 1,000 steps, a gradient descent-optimized TensorFlow model is
no better than a simple sklearn logistic regression. OK, that's fine, but what if we allowed
the model to iterate over the iris dataset even more?
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)
classifier =
tf.contrib.learn.LinearClassifier(feature_columns=feature_columns,
optimizer=optimizer,
n_classes=3)
classifier.fit(x=X_train,
y=y_train,
steps=2000)

# number of iterations is 2000 now

Our code is exactly the same as before, but now we have 2000 steps instead of 1000:
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
y=y_test)["accuracy"]

print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.973684

And now we have even better accuracy!
Note that you need to be very careful in choosing the number of steps. As
you increase this number, you increase the number of times your model
sees the exact same training points over and over again. We do have a
chance of overfitting! To remedy this, I would recommend choosing
multiple train/test splits and running the model on each one (k-fold cross
validation).
It is also worth mentioning that TensorFlow implements very low bias, high-variance
models, meaning that running the preceding code again for TensorFlow might result in a
different answer! This is one of the caveats of deep learning. They might converge to a very
great low bias model, but that model will have a high variance and, therefore, amazingly
might not generalize to all of the sample data. As mentioned before, cross validation would
be helpful in order to mitigate this.

[ 367 ]

Case Studies

Chapter 13

TensorFlow and neural networks
Now, let's point a more powerful model at our iris dataset. Let's create a neural network
whose goal it is to classify iris flowers (because why not?):
# Specify that all features have real-value data
feature_columns = [tf.contrib.layers.real_valued_column("", dimension=4)]
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier =
tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_units=[10, 20, 10],
optimizer=optimizer,
n_classes=3)
# Fit model.
classifier.fit(x=X_train,
y=y_train,
steps=2000)

Notice that our code really hasn't changed from the last segment. We still have our
feature_columns from before, but now we introduce, instead of a linear classifier, a
DNNClassifier, which stands for Deep Neural Network Classifier.
This is TensorFlow's syntax for implementing a neural network. Let's take a closer look:
tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_units=[10, 20, 10],
optimizer=optimizer,
n_classes=3)

We see that we are inputting the same feature_columns, n_classes, and optimizer,
but see how we have a new parameter called hidden_units? This list represents the
number of nodes to have in each layer between the input and the output layer.

[ 368 ]

Case Studies

Chapter 13

All in all, this neural network will have five layers:
The first layer will have four nodes, one for each of the iris feature variables, and
this layer is the input layer
A hidden layer of 10 nodes
A hidden layer of 20 nodes
A hidden layer of 10 nodes
The final layer will have three nodes, one for each possible outcome of the
network, and this is called our output layer
Now that we've trained our model, let's evaluate it on our test set:
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=X_test,
y=y_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.921053

Hmm, our neural network didn't do so well on this dataset, but perhaps it is because the
network is a bit too complicated for such a simple dataset. Let's introduce a new dataset
that has a bit more to it...
The mnist dataset consists of over 50,000 handwritten digits (0-9) and the goal is to
recognize the handwritten digits and output which letter they are writing. TensorFlow has
a built-in mechanism for downloading and loading these images. We've seen these images
before, but on a much smaller scale in Chapter 12, Beyond the Essentials:
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data/", one_hot=False)
Extracting
Extracting
Extracting
Extracting

MNIST_data/train-images-idx3-ubyte.gz
MNIST_data/train-labels-idx1-ubyte.gz
MNIST_data/t10k-images-idx3-ubyte.gz
MNIST_data/t10k-labels-idx1-ubyte.gz

Notice that one of our inputs for downloading mnist is called one_hot. This parameter
either brings in the dataset's target variable (which is the digit itself) as a single number, or
has a dummy variable.

[ 369 ]

Case Studies

Chapter 13

For example, if the first digit were a 7, the target would be either of these:
7: If one_hot was false
0 0 0 0 0 0 0 1 0 0: If one_hot was true (notice that starting from 0, the seventh
index is a 1)
We will encode our target the former way, as this is what our TensorFlow neural network
and our sklearn logistic regression will expect.
The dataset is split up already into a training and test set, so let's create new variables to
hold them:
x_mnist = mnist.train.images
y_mnist = mnist.train.labels.astype(int)

For the y_mnist variable, I specifically cast every target as an integer (by default, they
come in as floats) because otherwise, TensorFlow would throw an error at us.
Out of curiosity, let's take a look at a single image:
import matplotlib.pyplot as plt
plt.imshow(x_mnist[10].reshape(28, 28))

The number 0 in the MNIST dataset

[ 370 ]

Case Studies

Chapter 13

And, hopefully, our target variable matches at the 10th index as well:
y_mnist[10]
0

Excellent! Let's now take a peek at how big our dataset is:
x_mnist.shape
(55000, 784)
y_mnist.shape
(55000,)

Our training size then is 55000 images and target variables.
Let's fit a deep neural network to our images and see whether it will be able to pick up on
the patterns in our inputs:
# Specify that all features have real-value data feature_columns =
[tf.contrib.layers.real_valued_column("", dimension=784)] optimizer =
tf.train.GradientDescentOptimizer(learning_rate=.1) # Build 3 layer DNN
with 10, 20, 10 units respectively. classifier =
tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_units=[10, 20, 10], optimizer=optimizer, n_classes=10) # Fit model.
classifier.fit(x=x_mnist, y=y_mnist, steps=1000) # Warning this is
veryyyyyyyy slow

This code is very similar to our previous segment using DNNClassifier; however, look
how, in our first line of code, I have changed the number of columns to be 784 while, in the
classifier itself, I changed the number of output classes to be 10. These are manual inputs
that TensorFlow must be given to work.
The preceding code runs very slowly. It is adjusting itself, little by little, in order to get the
best possible performance from our training set. Of course, we know that the ultimate test
here is testing our network on an unknown test set, which is also given to us by
TensorFlow:
x_mnist_test = mnist.test.images
y_mnist_test = mnist.test.labels.astype(int)
x_mnist_test.shape
(10000, 784)

y_mnist_test.shape
(10000,)

[ 371 ]

Case Studies

Chapter 13

So we have 10,000 images to test on; let's see how our network was able to adapt to the
dataset:
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=x_mnist_test,
y=y_mnist_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.920600
Not bad, 92% accuracy on our dataset. Let's take a second and compare this
performance to a standard sklearn logistic regression now:
logreg = LogisticRegression()
logreg.fit(x_mnist, y_mnist)
# Warning this is slow
y_predicted = logreg.predict(x_mnist_test)
from sklearn.metrics import accuracy_score
# predict on our test set, to avoid overfitting!
accuracy = accuracy_score(y_predicted, y_mnist_test)
# get our accuracy score
accuracy
0.91969

Success! Our neural network performed better than the standard logistic regression. This is
likely because the network is attempting to find relationships between the pixels
themselves and using these relationships to map them to what digit we are writing down.
In logistic regression, the model assumes that every single input is independent of one
another, and therefore has a tough time finding relationships between them.

[ 372 ]

Case Studies

Chapter 13

There are ways of making our neural network learn differently:
We could make our network wider, that is, increase the number of nodes in the
hidden layers instead of having several layers of a smaller number of nodes:

Artiﬁcial neural network (Source: http://electronicdesign.com/site-ﬁles/electronicdesign.com/ﬁles/uploads/2015/02/0816_Development_Tools_F1_0.gif)

# A wider network
feature_columns = [tf.contrib.layers.real_valued_column("",
dimension=784)]
optimizer = tf.train.GradientDescentOptimizer(learning_rate=.1)

# Build 3 layer DNN with 10, 20, 10 units respectively.
classifier =
tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,
hidden_units=[1500],
optimizer=optimizer,
n_classes=10)
# Fit model.
classifier.fit(x=x_mnist,
y=y_mnist,
steps=100)
# Warning this is veryyyyyyyy slow
# Evaluate accuracy.
accuracy_score = classifier.evaluate(x=x_mnist_test,
y=y_mnist_test)["accuracy"]
print('Accuracy: {0:f}'.format(accuracy_score))
Accuracy: 0.898400

[ 373 ]

Case Studies

Chapter 13

We could increase our learning rate, forcing the network to attempt to converge
on an answer faster. As I mentioned before, we run the risk of the model
skipping the answer entirely if we go down this route. It is usually better to stick
with a smaller learning rate.
We can change the method of optimization. Gradient descent is very popular;
however, there are other algorithms for doing this. One example is called the
Adam Optimizer. The difference is in the way they traverse the error function,
and therefore the way that they approach the optimization point. Different
problems in different domains call for different optimizers.
There is no replacement for a good old fashioned feature selection phase, instead
of attempting to let the network figure everything out for us. We can take the
time to find relevant and meaningful features that actually will allow our
network to find an answer quicker!

Summary
In this chapter, we've seen three different case studies from three different domains using
many different statistical and machine learning methods. However, what all of them have
in common is that in order to solve them properly, we had to implement a data science
mindset. We had to solve problems in an interesting way, obtain data, clean the data,
visualize the data, and finally model the data and evaluate our thinking process.
I do hope that you have found the contents of this book to be interesting and not just the
final chapter! I leave it unto you to keep exploring the world of data science. Keep learning
Python. Keep learning statistics and probability. Keep your minds open. It is my hope that
this book has been a catalyst for you to go out and find even more on the subject.
For further reading beyond this book, I highly recommend looking into well-known data
science books and blogs, such as:
Dataschool.io, a blog by Kevin Markham
Python for Data Scientists, by Packt
If you would like to contact me for any reason, please feel free to reach out to
sinan.u.ozdemir@gmail.com.

[ 374 ]

14
Microsoft Azure Databricks
The main purpose of this chapter is to highlight the Microsoft Data Environment and how
we can utilize the many tools provided to us, especially Azure Databricks: a fullyfledged,
powerful analytics platform powered by Apache Spark.
We have three main case studies in this chapter that join together the principles of data
science and machine learning that we have learned about in this book with the ease and
power of the Microsoft Data Environment, specifically Azure Databricks. Each case study
will highlight different features of using Azure Databricks, as well as aspects of machine
learning that we have learned in this text.

The Microsoft data science environment
Microsoft offers many tools and environments to the data scientist, and this offering
consists of many parts. The three main components are as follows:
Microsoft Azure: This is an enterprise-grade cloud computing platform that
provides a great deal of access and support for production-ready scalable
systems.
Microsoft Azure Machine Learning Studio: This is a GUI-based environment for
creating and operationalizing a machine learning workflow that is optimized for
Azure.
Azure Databricks: This is an Apache Spark-based analytics and machine
learning platform optimized for the Microsoft Azure platform.

Microsoft Azure Databricks

Chapter 14

This text will focus heavily on the Azure Databricks environment as it provides access to
many tools, including these:
Spark DataFrames: Spark DataFrames are distributed collections of data
organized into rows and columns. They are conceptually equivalent to a data
frame in Python.
Notebooks: Like Jupyter, the Azure Databricks notebook tool provides a cellbased code environment for writing logically separated code that is easy to read
and replicate.
Clusters/workers: Using Microsoft Azure (or another cloud computing
platform), we can spin up resources in order to massively optimize and
parallelize our machine learning and data analysis to allow for faster iteration.
MLib: This is a scalable machine learning library consisting of common learning
algorithms and utilities, including classification, regression, and more.
Azure Databricks provides many more capabilities other than what we have listed here. We
will focus on the tools that are relevant to us as data scientists and machine learning
engineers.
For more information on Azure Databricks, check out https:/​/​docs.
microsoft.​com/​en-​us/​azure/​azure-​databricks/​what-​is-​azuredatabricks.

What exactly are Spark and PySpark?
The backbone of Azure Databricks is Apache Spark. Spark is an analytics engine for big
data processing. It has built-in modules for data streaming, SQL, machine learning, and
more. PySpark is the Python API for Spark. It allows us to invoke the power of Spark using
Python code. Azure Databricks brings all of this at our fingertips and makes setting up
clusters running Spark and PySpark within seconds possible. Let's see it in action!

Basic Azure Databricks use
Before we begin with our case studies, it is important to get our bearings in Azure
Databricks. We will begin by setting up our first cluster and some notebooks to write our
Python code in.

[ 376 ]

Microsoft Azure Databricks

Chapter 14

Setting up our first cluster
To get started in Azure Databricks, we have to set up our first cluster. This will spin up
(initialize) resources running the Azure Databricks Runtime Environment (including
Spark). This is where all of the action takes place. Whenever we run code in our notebooks,
the code is sent to our cluster to actually run it. This means that no code will ever actually
run on our local machine. This is great for many reasons, one of the main ones being that it
provides data scientists with sub-optimal equipment at home/work a chance to use
production-quality resources for a fraction of the cost!
To set up a cluster, navigate to the Clusters option on the left-hand pane and click on
Create cluster. There, you will see a form with three basic pieces of information: Cluster
Name, Azure Databricks Runtime Version, and Python Version. Make your cluster name
whatever your heart desires. Try to use the most recent Azure Databricks version (this is
the default) and select Python version 3. Once that's finished, click Create cluster again and
you are done.
When spinning up a cluster, there are many optional advanced settings
that we have the ability to set. For our purposes, we will not need to.

The page should look something like this:

[ 377 ]

Microsoft Azure Databricks

Chapter 14

Once we have a cluster, it is time to set up a notebook. This can be done from the home
page. When creating a new notebook, all we have to do is select the language we wish to
use (Python for now, but more are available). The notebooks in Azure Databricks are nearly
identical to Jupyter notebooks in functionality and use (nifty!). This comes in handy for
data scientists who are used to this environment:

Creating a notebook is even easier than spinning up a new cluster. All of the code that we write in here will be run on our cluster and not on our local machine

Once we have our cluster up and running and we are able to create notebooks, it is time to
jump right into using the Azure Databricks environment and seeing first-hand how the
power of Apache Spark and the Microsoft Data Environment will affect the way that we
write data-driven code.

Case study 1 – bike-sharing usage prediction
using parallelization in Azure Databricks
Our first case study will focus on setting up a simple notebook in Azure Databricks and
running some basic data visualization and machine learning code in order to get used to
the Azure Databricks environment. Azure Databricks comes with a built-in filesystem that
is preloaded with data for us to use. We can upload our own files to the system (which we
will do in the second case study), but for now, we will import a dataset that came preloaded with Azure Databricks. We will also make heavy use of the built-in Spark-based
visualization tools to analyze our data to the fullest extent that we can.

[ 378 ]

Microsoft Azure Databricks

Chapter 14

The aspects of Azure Databricks that we will be highlighting in this case study include the
following:
The collection of open data that is easily accessible by the Azure Databricks
filesystem
Converting our pandas DataFrames to Spark equivalents and generating
visualizations
Parallelizing some simple hyperparameter tuning
Broadcasting variables to workers to enhance parallelization further
The data that we will be using involves predicting the amount of bikes being rented via a
bike-share system. Our goal is to predict the usage of the system based on daily/hourly
corresponding weather, time-based, and seasonal information. Let's get right to it and see
our first Azure Databricks-specific programming. We can access this through the dbutils
module:
# display is a reserved function in Databricks that allows us to view and
manipulate Dataframes inline
# dbutils is a library full of ready-to-use datasets for us to use
# Let's start by displaying all of the directories in the main data folder
# "databricks-datasets"
display(dbutils.fs.ls("/databricks-datasets"))

The output is a DataFrame of available folders to look in for data. Here is a snippet:
path
dbfs:/databricks-datasets/README.md
dbfs:/databricks-datasets/Rdatasets/
dbfs:/databricks-datasets/SPARK_README.md
dbfs:/databricks-datasets/adult/
dbfs:/databricks-datasets/airlines/
dbfs:/databricks-datasets/amazon/
dbfs:/databricks-datasets/asa/
dbfs:/databricks-datasets/atlas_higgs/
dbfs:/databricks-datasets/bikeSharing/
dbfs:/databricks-datasets/cctvVideos/
dbfs:/databricks-datasets/credit-card-fraud/
dbfs:/databricks-datasets/cs100/
dbfs:/databricks-datasets/cs110x/
...

[ 379 ]

name
README.md
Rdatasets/
SPARK_README.md
adult/
airlines/
amazon/
asa/
atlas_higgs/
bikeSharing/
cctvVideos/
credit-card-fraud/
cs100/
cs110x/

size
976
0
3359
0
0
0
0
0
0
0
0
0
0

Microsoft Azure Databricks

Chapter 14

Every time we run a command in our notebook, the time that it took our cluster to execute
that code is shown at the bottom. For example, this code block took my cluster 0.88 seconds.
In general, the format for this statement is as follows:
Command took <<elapsed_time>> -- by <<username/email>> at <<date>>,
<<time>> on <<cluster_name>>

We can view the contents of a particular file:
# Let's check out the general README of the date folder by opening the
markdown folder and printing out the result of "readlines"
# which is a way to print out the contents of a file
with open("/dbfs/databricks-datasets/README.md") as f:
x = ''.join(f.readlines())
print(x)
Databricks Hosted Datasets
==========================
The data contained within this directory is hosted for users to build data
pipelines using Apache Spark and Databricks.
License
------....

Let's take a deeper look at the bikeSharing directory:
# Let's list the contents of the directory corresponding to the data that
we want to import
display(dbutils.fs.ls("dbfs:/databricks-datasets/bikeSharing/"))

Running the preceding code will list out the contents of the bikeSharing directory:
path
dbfs:/databricks-datasets/bikeSharing/README.md
dbfs:/databricks-datasets/bikeSharing/data-001/

name
README.md
data-001/

size
5016
0

We have a README file in markdown format and another sub-directory. In the data-001
folder, we can list out the contents and see that there are two CSV files:
# the data is given to use in both an hourly and a daily format.
display(dbutils.fs.ls("dbfs:/databricks-datasets/bikeSharing/data-001/"))

[ 380 ]

Microsoft Azure Databricks

Chapter 14

The output is as follows:
path
dbfs:/databricks-datasets/bikeSharing/data-001/day.csv
dbfs:/databricks-datasets/bikeSharing/data-001/hour.csv

name
day.csv
hour.csv

size
57569
1156736

We will use the hourly format for our study. Before we do, note that there is a README in
the directory that will give us context and information about the data. We can view the file
by opening it and printing our the line contents as we did before:
# Note that we had to change the format from dbfs:/ to /dbfs/
with open("/dbfs/databricks-datasets/bikeSharing/README.md") as f:
x = ''.join(f.readlines())
print(x)
## Dataset
Bike-sharing rental process is highly correlated to the environmental and
seasonal settings. For instance, weather conditions, precipitation, day of
week, season, hour of the day, etc. can affect the rental behaviors. The
core data set is related to the two-year historical log corresponding to
years 2011 and 2012 from Capital Bikeshare system, Washington D.C., USA
which is publicly available in http://capitalbikeshare.com/system-data. We
aggregated the data on two hourly and daily basis and then extracted and
added the corresponding weather and seasonal information. Weather
information are extracted from http://www.freemeteo.com.
...

The README goes on to say that this dataset is primarily used to do regression (predicting
a continuous response). We will treat the problem as a classification by bucketing our
response to classes. More on this later. Let's import our data into a good old-fashioned
pandas DataFrame:
# Load data into a Pandas dataframe (note that pandas, sklearn, etc come
with the environment. That's pretty neat!)
import pandas
bike_data = pandas.read_csv("/dbfs/databricksdatasets/bikeSharing/data-001/hour.csv").iloc[:,1:] # remove line number
# view the dataframe
#look at the first row
bike_data.loc[0]

[ 381 ]

Microsoft Azure Databricks

Chapter 14

The output is as follows:
dteday 2011-01-01
season 1
yr 0 mnth 1 hr 0 holiday 0 weekday 6 workingday 0 weathersit 1 temp 0.24
atemp 0.2879 hum 0.81 windspeed 0 casual 3 registered 13 cnt 16

Descriptions for each variable are in the README:
holiday: A Boolean that is 1 (True) means the day is a holiday and 0 (False)

means it is not (extracted from http:/​/​dchr.​dc.​gov/​page/​holiday-​schedule)
workingday: A Boolean that is 1 (True) means the day is neither a weekend nor
a holiday, otherwise 0 (False)
cnt: Count of total rental bikes, including both casual and registered
The README included in the Azure Databricks filesystem is very helpful for context and
information about the datasets. Let's see how many observations we have:
# 17,379 observations
bike_data.shape
(17379, 16)

It is nice to stop and notice that everything that we have done so far (except for the Azure
Databricks filesystem module) would be exactly the same if we were to do it in a normal
Jupyter environment. Let's see whether our dataset has any missing data that we would
have to deal with:
# no missing data, great!
bike_data.isnull().sum()
dteday 0
season 0
yr 0
mnth 0 hr 0
holiday 0
weekday 0
workingday 0
weathersit 0
temp 0
atemp 0
hum 0
windspeed 0
casual 0
registered 0
cnt 0

[ 382 ]

Microsoft Azure Databricks

Chapter 14

No missing data. Excellent. Let's see a quick histogram of the cnt column, as it represents
the total count of bikes reserved in that hour:
# not everything will carry over, but that is ok
%matplotlib inline
bike_data.hist("cnt")
# matplotlib inline is not supported in Databricks.
# You can display matplotlib figures using display(). For an example, see #
https://docs.databricks.com/user-guide/visualizations/matplotlib-and-ggplot
.html

We get an error when we run this code, even though we have run this command in this
book before. Unfortunately, matplotlib does not work exactly the same in this
environment as it does in Jupyter. That is fine, though, because Azure Databricks provides
a visualization tool built on top of Spark's version of a DataFrame that puts even more
capabilities at our fingertips. To get access to these visualizations, we will have to first
convert our pandas DataFrame to its Spark equivalent:
# converting to a Spark DataFrame allows us to make multiple types of plots
using either a sample of data or the entire data population as the source
sparkDataframe = spark.createDataFrame(bike_data)
# display is a simple way to view your data either via a table or through
graphs
display(sparkDataframe)

Running the preceding code yields a snippet of the Dataframe for us to inspect:

[ 383 ]

Microsoft Azure Databricks

Chapter 14

The display command shows the Spark DataFrame inline with our notebook.
Azure Databricks allows for extremely powerful graphing capabilities thanks to
Spark. When using the display command, on the bottom-left of the cell, a widget will
appear, allowing us to graph using the data. Let's click on the Histogram:

Azure Databricks oﬀers a multitude of graphing options to get the best picture of our data

Another button will appear, called Plot Options..., which allows us to customize our graph.
We can drag and drop our columns into one of three fields:
Keys will, in general, represent our x-axis.
Values will, in general, represent our y-axis.
Series groupings will separate our data into groupings in order to get a bigger
picture.

[ 384 ]

Microsoft Azure Databricks

Chapter 14

By default, display and visualizations will only aggregate over the first
1,000 rows for convenience. We can force Azure Databricks to utilize the
entire dataset in our graph by setting the appropriate option.

Creating a simple histogram in Azure Databricks couldn't be simpler!

We can see how easy to use the system is and that our distribution is right-skewed.
Interesting! Let's try something else now. Let's say we also want to visualize the hourly
usage, separated by the binary variable workingday; the idea being that we are curious to
see how the total amount of bikes reserved changes by the hour and if that distribution
changes depending on whether it's a working day or not. We can achieve this by selecting
cnt as our value, hr as our key, and workingday as our grouping.

[ 385 ]

Microsoft Azure Databricks

Chapter 14

It should look something like this:

Hitting Apply will apply this to the entire dataset, instead of the first-1,000-row sample that
it shows us on the right:

[ 386 ]

Microsoft Azure Databricks

Chapter 14

It is extremely easy to use Azure Databricks to generate beautiful and interpretable graphs
based on our data. From this, we can see that bike sharing appears to be somewhat
normally distributed on weekends while workdays have large spikes in the morning and
evening (which makes sense).
In this dataset, there are three possible regression response candidates: casual,
registered, and cnt. We will turn our problem into a classification problem by bucketing
the cnt column into one of two buckets. Our response will either be 100 or fewer bikes
reserved that hour (False) or over 100 (True):
# Seperate into X, and y (features and label)
# we will turn our regression problem into a classification problem by
bucketing the column "cnt" as being either over 100 / 100 or under.
features, labels = bike_data[["season", "yr", "mnth", "hr", "holiday",
"weekday", "workingday", "weathersit", "atemp", "hum", "windspeed"]],
bike_data["cnt"] > 100
# See the distribution of our labels
labels.value_counts()
True 10344
False 7035

Let's now create a simple function that will do a few things: take in a param choice for a
random forest classifier, fit and test our model on our data, and return the results:
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
#
#
#
#
#
#
#

Create a function that will
0. Take in a parameter for our Random Forest's n_estimators param
1. Instantiate a Random Forest algorithm
2. Split our data into a training and testing split
3. Fit on our training data
4. evaluate on our testing data
5. return a tuple with the n_estimators param and corresponding accuracy

def runRandomForest(c):
rf = RandomForestClassifier(n_estimators=c)
# Split into train and test using
sklearn.cross_validation.train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, labels,
test_size=0.2, random_state=1)

[ 387 ]

Microsoft Azure Databricks

Chapter 14

# Build the model
rf.fit(X_train, y_train)
# Calculate predictions and accuracy
predictions = rf.predict(X_test)
accuracy = accuracy_score(predictions, y_test)
# return param and accuracy score
return (c, accuracy)

This function takes in a single number as an input, sets that number as a random forest's
n_estimator parameter, and returns the accuracy associated with that param choice on a
train-test split:
runRandomForest(1)
(1, 0.90218642117376291)
Command took 0.13 seconds

This took a very short amount of time because we were only training a single decision tree
in our forest. Let's now iteratively try a varying number of estimators and see how this
affects our accuracy:
for i in [1, 10, 100, 1000, 10000]:
print(runRandomForest(i))
(1, 0.89700805523590332)
(10, 0.939873417721519)
(100, 0.94677790563866515)
(1000, 0.94677790563866515)
(10000, 0.94792865362485612)

Command took 4.16 minutes

It took my cluster over 4 minutes to try these five n_estimator options. Most of the time
was taken up by the final two options as it took a very long time to train thousands of
decision trees.
As we add more combinations of parameters and more parameter
options, the time it will take to iteratively go through these options will
explode. We will see how we can utilize Databrick's environment to
optimize this in the third case study.

[ 388 ]

Microsoft Azure Databricks

Chapter 14

Let's make use of Spark to parallelize our for loop. Every notebook has a special variable
called sc that represents Spark:
# every notebook has a variable called "sc" that represents the Spark
context in our cluster
sc

There are a few ways of performing this parallelization, but in general, it will look like this:
1. Create a dataset that will be sent to our cluster (in this case, parameter options).
2. Map a function to each element of the dataset (our runRandomForest function).
3. Collect the results:
# 1. set up 5 tasks in our Spark Cluster by parallelizing a
dataset (list) of five elements (n_estimator options)
k = sc.parallelize([1, 10, 100, 1000, 10000])
# 2. map our function to our 5 tasks
# The code will not be sent to our cluster until we run the
next command
results = k.map(runRandomForest)
Command took 0.13 seconds

Here, we are introduced to our first Apache Spark-specific syntax. Step 1 will return a
distributed dataset that is optimized for parallel computation, and we will call that dataset
k. The values of k represent different possible arguments for our function,
runRandomForest. Step 2 tells our cluster to run the function across our distributed
dataset.
It is important to note that while Step 1 and Step 2 are Spark-specific commands, up until
now, our function has not actually been sent to our cluster for execution. We have just set
the stage to do so by setting up the appropriate variables. Step 3 will collect our results by
running the function in parallel across the different values in k:
# 3. the collect method actually sends the five tasks to our cluster for
execution
# Faster (1.5x) because we aren't doing each task one after the other. We
are doing them in parallel
# This becomes much more noticeable when doing more params (we will get to
this in a later case study)
results.collect()
Command took 2.73 minutes

[ 389 ]

Microsoft Azure Databricks

Chapter 14

Immediately, we can see the value of parallelizing functions using Spark. By doing nothing
more than relying on Azure Databricks and Spark, we are able to perform our for loop 1.5x
faster. If we use a variable in a function (like our dataset), Spark will automatically send the
dataset to the workers. This is usually fine. We can send it to workers more efficiently by
broadcasting it. By broadcasting data, a copy of the data is sent to our workers, which are
used when running tasks. This is much more efficient when dealing with extremely large
datasets with large values in them. We can rewrite our previous function using broadcast
variables:
# Broadcast dataset
# If we use a variable in a function, Spark will automatically send the
dataset to the workers. This is usually fine.
# We can send it to workers more efficiently by broadcasting it. By
broadasting data, a copy is sent to our workers which are used when running
tasks.
# For more info on broadcast variables, see the Spark programming guide.
You can create a Broadcast variable using sc.broadcast().
# To access the value of a broadcast variable, you need to use .value
# broadcast the variables to our workers
featuresBroadcast = sc.broadcast(features)
labelsBroadcast = sc.broadcast(labels)
# reboot of the previous function
def runRandomForestBroadcast(c):
rf = RandomForestClassifier(n_estimators=c)
# Split into train and test using
sklearn.cross_validation.train_test_split
# ** This part of the function is the only difference from the previous
version **
X_train, X_test, y_train, y_test =
train_test_split(featuresBroadcast.value, labelsBroadcast.value,
test_size=0.2, random_state=1)
# Build the model
rf.fit(X_train, y_train)
# Calculate predictions and accuracy
predictions = rf.predict(X_test)
accuracy = accuracy_score(predictions, y_test)
return (c, accuracy)

[ 390 ]

Microsoft Azure Databricks

Chapter 14

Once our new function using broadcast variables is complete, running it in parallel is no
different:
# set up 5 tasks in our Spark Cluster
k = sc.parallelize([1, 10, 100, 1000, 10000])
# map our function to our five tasks
results = k.map(runRandomForestBroadcast)
# the real work begins here.
results.collect()

The timing is not very different from our last run with non-broadcast data. This is because
our data and logic are not large enough to see a noticeable difference yet. Once done with
our variables, we can unpersist (unbroadcast) them like so:
# Since we are done with our broadcast variables, we can clean them up.
# (This will happen automatically, but we can make it happen earlier by
explicitly unpersisting the broadcast variables.
featuresBroadcast.unpersist()
labelsBroadcast.unpersist()

Hyperparameter tuning would be very difficult to do if you wanted to do a grid search
across multiple parameters and multiple options. We could enhance our function in order
to accommodate this; however, it would be better to rely on existing frameworks in scikitlearn to do so. We will see how to remedy this conundrum in a later case study.
We can see how we can utilize Databrick's easy-to-use environment, clusters, and
notebooks in order to enhance our data analysis and machine learning with minimal
changes to our coding style. In the next case study, we will examine Spark's scalable
machine learning library, MLlib, for optimized machine learning speed.

Case study 2 – Using MLlib in Azure Databricks
to predict credit card fraud
Our second case study will focus on predicting credit card fraud and will make use of
MLlib, Apache Spark's scalable machine learning library. MLlib comes standard with our
Azure Databricks environment and allows us to write scalable machine learning code. This
case study will focus on MLlib syntax while we draw parallels (no pun intended) to its
scikit-learn cousins.

[ 391 ]

Microsoft Azure Databricks

Chapter 14

The aspects of Azure Databricks that we will be highlighting in this case study include
these:
Importing a CSV that is uploaded to the Azure Databricks filesystem
Using MLlib's pipeline, feature pre-processing, and machine learning library to
write scalable machine learning code
Using MLlib metric evaluation modules that mirror scikit-learn's components
The data in question is predicting credit card fraud. The data represents credit card
transactions and contains 28 anonymized continuous features (called V1 .. V28) plus the
amount that the transaction was for and the time at which the transaction occurred. We will
not dive too deeply into feature engineering in this case study, but will focus mainly on
using the MLlib modules that come with Azure Databricks to predict the response label
(which is a binary variable). Let's get right into it and start by importing a CSV that we have
uploaded using the Azure Databricks data import feature:
# File location and type
# This dataset also exists in the DBFS of Databricks. This is just to show
how to import a CSV that has been previously uploaded so that
# you can upload your own!
file_location = "/FileStore/tables/creditcard.csv"
file_type = "csv"
# CSV options
# will automatically cast columns as appropiate types (float, string, etc)
infer_schema = "true"
first_row_is_header = "true"
delimiter = ","
# read a csv file and convert to a Spark dataframe
df = spark.read.format(file_type) \
.option("inferSchema", infer_schema) \
.option("header", first_row_is_header) \
.option("sep", delimiter) \
.load(file_location)
# show us the Spark Dataframe
display(df)

[ 392 ]

Microsoft Azure Databricks

Chapter 14

Our goal is to build a pipeline that will do this:
1.
2.
3.
4.
5.
6.

Assemble the columns that we wish to use as features
Scale our features using a standard z-score function
Encode our label as 0 or 1 (it already is but it is good to see this functionality)
Run a logistic regression across the training data to fit coefficients
Evaluate binary metrics on the testing set
Use an MLlib cross-validating grid searching module to find the best parameters
for our logistic regression model

Phew. That's a lot, so let's go one step at a time. The following code block will handle Step 1,
Step 2, and Step 3 by setting up a pipeline with three steps in it:
# import the pipeline module from pyspark
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler, StandardScaler,
StringIndexer
# will hold the steps of our pipeline
stages = []

The first stage of our pipeline will assemble the features that we want to use in our machine
learning procedure. Let's use all 28 entries from the anonymized data (V1 - V28) plus the
amount column. For this case study, we will not use the time column:
# Transform all features into a vector using VectorAssembler
# create list of ["V1", "V2", "V3", ...]
numericCols = ["V{}".format(i) for i in range(1, 29)]
# Add "Amount" to the list of features
assemblerInputs = numericCols + ["Amount"]
# VectorAssembler acts like scikit-learn's "FeatureUnion" to put together
the feature columns and adding the label "features"
assembler = VectorAssembler(inputCols=assemblerInputs,
outputCol="features")
# add the VectorAssembler to the stages of our MLLib Pipeline
stages.append(assembler)

The second stage of our pipeline will take the assembled features and scale them:
# MLLib's StandardScaler acts like scikit-learn's StandardScaler
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures",
withStd=True, withMean=True)
# add StandardScaler to our pipeline
stages.append(scaler)

[ 393 ]

Microsoft Azure Databricks

Chapter 14

Once we've gathered our features and scaled them, the final step of our pipeline encodes
our class label to ensure consistency:
# Convert label into label indices using the StringIndexer (like scikitlearn's LabelEncoder)
label_stringIdx = StringIndexer(inputCol="Class", outputCol="label")
# add the StringIndexer to our pipeline
stages.append(label_stringIdx)

That was a lot, but note that every step of the pipeline has a scikit-learn equivalent. As a
data scientist, the most important thing to remember is that as long as you know the theory
and the proper steps of data science and machine learning, you can transfer your skills
across many languages, platforms, and technologies. Now that we have created our list of
three stages, let's instantiate a pipeline object:
# create our pipeline with three stages
pipeline = Pipeline(stages=stages)

To make sure that we are training a model that is able to predict unseen cases, we should
split up our data into a training and testing set, and fit our pipeline to the training set while
using the trained pipeline to transform the testing set:
# We need to split our data into training and test sets. This is like
scikit-learn's train_test_split function
# We will also set a random number seed for reproducibility
(trainingData, testData) = df.randomSplit([0.7, 0.3], seed=1)
print(trainingData.count())
print(testData.count())
199470 # elements in the training set
85337 # elements in the testing set

Let's now fit our pipeline to the training set. This will learn the features as well as the
parameters to scale future unseen testing data:
# fit and transform to our training data
pipelineModel = pipeline.fit(trainingData)
trainingDataTransformed = pipelineModel.transform(trainingData)

[ 394 ]

Microsoft Azure Databricks

Chapter 14

If we take a look at our DataFrame, we will notice that we have three new columns:
features, scaledFeatures, and label. These three columns were added by our pipeline
and those names can be found exactly in the preceding code where we set the three stages
of the pipeline. Note that the data types of the features and scaledFeatures column
are vector. This indicates that they represent observations to be learned by our machine
learning model in the future:
# note the new columns "features", "scaledFeatures", and "label" at the end
trainingDataTransformed
Time:decimal(10,0)
V1:double
V2:double
V3:double
V4:double
V5:double
V6:double
V7:double
V8:double
V9:double
V10:double
V11:double
V12:double
V13:double
V14:double
V15:double
V16:double
V17:double
V18:double
V19:double
V20:double
V21:double
V22:double
V23:double
V24:double
V25:double
V26:double
V27:double
V28:double
Amount:double
Class:integer
features:udt
scaledFeatures:udt
label:double

[ 395 ]

Microsoft Azure Databricks

Chapter 14

Just like we do in scikit-learn, we will have to import out logistic regression model,
instantiate it, and fit it to our training data:
# Import the logistic regression module from pyspark
from pyspark.ml.classification import LogisticRegression
# Create initial LogisticRegression model
lr = LogisticRegression(labelCol="label", featuresCol="scaledFeatures")
# Train model with Training Data
lrModel = lr.fit(trainingDataTransformed)

This process should look very familiar to us as it is nearly identical to scikit-learn. The main
difference is that when we instantiate our model, we have to tell the object the names of the
label and features, instead of feeding the features and label separately into the fit method
(like we do in scikit-learn).
Once we fit our model, we can transform and gather predictions from our testing data:
# transform (not fit) to the testing data
testDataTransformed = pipelineModel.transform(testData)
# run our logistic regression over the transformed testing set
predictions = lrModel.transform(testDataTransformed)

Transforming our testing data using logistic regression will actually add three new columns
(like the pipeline did). We can see this by running this:
predictions:
Time: decimal(10,0)
V1: double
V2: double
V3: double
V4: double
V5: double
V6: double
V7: double
V8: double
V9: double
V10: double
V11: double
V12: double
V13: double
V14: double
V15: double
V16: double
V17: double
V18: double

[ 396 ]

Microsoft Azure Databricks

Chapter 14

V19: double
V20: double
V21: double
V22: double
V23: double
V24: double
V25: double
V26: double
V27: double
V28: double
Amount: double
Class: integer
features: udt
scaledFeatures: udt
label: double
rawPrediction: udt
probability: udt
prediction: double

Let's take a look at thelabel column as well as two of the new columns, probability and
prediction. We can do this by invoking the filter method of the Spark DataFrame:
selected = predictions.select("label", "prediction", "probability")
display(selected)

The output is as follows:
label
0
0
0
0
0
0
0
...

prediction
0
0
0
0
0
0
0

probability
[1,2,[],[0.9998645390717447,0.000135460928255428]]
[1,2,[],[0.9998821292706751,0.00011787072932487872]]
[1,2,[],[0.9994714991454193,0.000528500854580697]]
[1,2,[],[0.9991193503498385,0.0008806496501614437]]
[1,2,[],[0.9997043818469743,0.00029561815302580084]]
[1,2,[],[0.9998106820888389,0.00018931791116114655]]
[1,2,[],[0.9995735877526569,0.0004264122473429876]]

The label and prediction columns show each observation's ground truth and our
model's estimate, while the probability column holds a vector that contains the
predicted probability (think scikit-learn's predict_proba functionality). Let's now bring in
PySpark's metric evaluation module in order to get some basic metrics. we will start with
BinaryClassificationEvaluator, which can tell us the testing AUC (area under the
ROC curve):
# like scikit-learn's metric module
from pyspark.ml.evaluation import BinaryClassificationEvaluator

[ 397 ]

Microsoft Azure Databricks

Chapter 14

# Evaluate model using either area under Precision Recall curev or the area
under the ROC
evaluator = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction",
metricName="areaUnderROC")
evaluator.evaluate(predictions)
0.984986959421

This is helpful, but we also want some of the more familiar and interpretable metrics such
as accuracy, precision, recall, and more. We can get these using
PySpark's MulticlassMetrics module:
# Get some deeper metrics out of our predictions
from pyspark.mllib.evaluation import MulticlassMetrics
# must turn DF into RDD (Resilient Distributed Dataset)
predictionAndLabels = predictions.select("label",
"prediction").rdd.map(tuple)
# instantiate a MulticlassMetrics object
metrics = MulticlassMetrics(predictionAndLabels)
# Overall statistics
accuracy = metrics.accuracy
precision = metrics.precision()
recall = metrics.recall()
f1Score = metrics.fMeasure()
print("Summary Stats")
print("Accuracy = %s" % accuracy)
print("Precision = %s" % precision)
print("Recall = %s" % recall)
print("F1 Score = %s" % f1Score)

The output is as follows:
Summary Stats
Accuracy = 0.999203159239
Precision = 0.999203159239
Recall = 0.999203159239
F1 Score = 0.999203159239

Note that accuracy is an attribute of the MulticlassMetrics object and
not a method, so we do not need the parentheses.

[ 398 ]

Microsoft Azure Databricks

Chapter 14

Great! We can also calculate our true positive rate, false negative rate, and other by using
the predictions vector. This will give us a better sense of how our machine learning model
performs on particular examples of positive and negative fraud cases:
tp = predictions[(predictions.label
1)].count()
tn = predictions[(predictions.label
0)].count()
fp = predictions[(predictions.label
1)].count()
fn = predictions[(predictions.label
0)].count()
print
print
print
print

== 1) & (predictions.prediction ==
== 0) & (predictions.prediction ==
== 0) & (predictions.prediction ==
== 1) & (predictions.prediction ==

("True Positives:", tp)
("True Negatives:", tn)
("False Positives:", fp)
("False Negatives:", fn)

The output is as follows:
True Positives: 93
True Negatives: 85176
False Positives: 18
False Negatives: 50

Let's consider this our baseline logistic regression model and try to optimize our results by
tweaking our logistic regression parameters.

Using the MLlib Grid Search module to tune
hyperparameters
To optimize our parameters, it would help to know exactly what they were and how to use
them. Luckily, we can see the README in each model by running the explainParams
method. This will generate a list of the available parameters, a description as to what they
represent, and usually a guide to the acceptable values:
# explain the parameters that are included in MLLib's Logistic Regression
print(lr.explainParams())

The output is as follows:
aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For
alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1

[ 399 ]

Microsoft Azure Databricks

Chapter 14

penalty. (default: 0.0)
family: The name of family which is a description of the label distribution
to be used in the model. Supported options: auto, binomial, multinomial
(default: auto) featuresCol: features column name. (default: features,
current: scaledFeatures) fitIntercept: whether to fit an intercept term.
(default: True)
labelCol: label column name. (default: label, current: label)
....

Let's build a parameter grid for Spark to gridsearch across. Let's choose three parameters
to start with: maxIter, regParam, and elasticNetParam. We first need to build a
parameter grid using PySpark's version of scikit-learn's GridSearchCV:
# pyspark's version of GridSearchCV
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
# Create ParamGrid for Cross Validation
paramGrid = (ParamGridBuilder()
.addGrid(lr.regParam, [0.0, 0.01, 0.5, 2.0])
.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])
.addGrid(lr.maxIter, [5, 10])
.build())

# Create 5-fold CrossValidator that can also test multiple parameters (like
scikit-learn's GridSearchCV)
cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid,
evaluator=evaluator, numFolds=5)

Let's send our grid search to the cluster for the efficient parallelization of tasks:
# Run cross validations on our cluster
cvModel = cv.fit(trainingDataTransformed)
# this will likely take a fair amount of time because of the amount of
models that we're creating and testing

The variable, cvModel, holds the logistic regression model with the optimized parameters.
From our optimized model, we can extract the learned coefficients to gain a deeper
understanding as to how the features correlate to the label:
# extract the weights from our model
weights = cvModel.bestModel.coefficients
# convert from numpy type to float
weights = [[float(w)] for w in weights]
weightsDF = sqlContext.createDataFrame(weights, ["Feature Weight"])
display(weightsDF)

[ 400 ]

Microsoft Azure Databricks

Chapter 14

Running the preceding code yields a single-column DataFrame with the logistic regression
coefficients. As we've seen before, the weights represent the importance and correlation
between the features and the response:
Feature Weight
-0.009310428799214699
0.024089617982041803
-0.07660344812402071
0.13879375587420806
0.03389644146602658
-0.03197804822203382
-0.026671134727093863
-0.05963860645699601
-0.07157334685249503
-0.12739634200985744
0.11271203988538568
-0.16991941687681994
-0.022382161065846975
-0.2967372927422323
-0.002525484797586701
-0.08661888759753078
-0.09428351861530046
-0.010145267312697291
0.000474661023205239
0.003985147929831393
0.031230406955806467
0.011362000753207976
-0.014548646956536248
-0.011270335506048019
-0.004390342109545349
0.008722583938741943
0.01390573346423987
0.014176539525542918
0.021489763526114244

We can grab our predictions in the same way we did previously to compare our results:
# Use test set to get the best params
predictions = cvModel.transform(testDataTransformed)
# must turn DF into RDD (Resilient Distributed Dataset)
predictionAndLabels = predictions.select("label",
"prediction").rdd.map(tuple)
metrics = MulticlassMetrics(predictionAndLabels)
# Overall statistics

[ 401 ]

Microsoft Azure Databricks

Chapter 14

accuracy = metrics.accuracy
precision = metrics.precision()
recall = metrics.recall()
f1Score = metrics.fMeasure()
print("Summary Stats")
print("Accuracy = %s" % accuracy)
print("Precision = %s" % precision)
print("Recall = %s" % recall)
print("F1 Score = %s" % f1Score)

The output is as follows:
Summary Stats
Accuracy = 0.998839893598
Precision = 0.998839893598
Recall = 0.998839893598
F1 Score = 0.998839893598

Run the following code:
tp = predictions[(predictions.label
1)].count()
tn = predictions[(predictions.label
0)].count()
fp = predictions[(predictions.label
1)].count()
fn = predictions[(predictions.label
0)].count()

== 1) & (predictions.prediction ==
== 0) & (predictions.prediction ==
== 0) & (predictions.prediction ==
== 1) & (predictions.prediction ==

print "True Positives:", tp
print "True Negatives:", tn
print "False Positives:", fp
print "False Negatives:", fn
# False positive went from 18 to 16 (win) but False Negative jumped to 83
(opposite of win)

The output is as follows:
True Positives: 67
True Negatives: 85178
False Positives: 16
False Negatives: 76

It's easy to see how Azure Databricks' environment makes it easy to utilize Spark and
MLlib to create scalable machine learning pipelines that are similar in construction and
usage to scikit-learn.

[ 402 ]

Microsoft Azure Databricks

Chapter 14

Case study 3 – Using Azure Databricks to
optimize our hyperparameter tuning
Our final case study is the shortest and will showcase how we can combine the best of
scikit-learn, Spark, and Azure Databricks to build simple yet powerful machine learning
models. We will be using the MNIST dataset, which we used earlier, and we will be fitting
a fairly simple RandomForestClassifier to the data. The interesting bit will come when
we import a third-party tool called spark_sklearn to help us out.
The aspects of Azure Databricks that we will be highlighting in this case study include
these:
Importing third-party packages into our Azure Databricks environment
Enabling Spark's parallelization within scikit-learn's easy-to-use syntax

How to add Python libraries to your cluster
Up until now, all of the packages that we have used come with the Azure Databricks
environment. Now we need to add a new package, called spark_sklearn. To add a thirdparty package to our Azure Databricks cluster, we simply click on Import Library from our
main dashboard, and we will see a window like the following:

Type in any package that you wish to use in the PyPi Name ﬁeld

[ 403 ]

Microsoft Azure Databricks

Chapter 14

We simply type in spark_sklearn, hit Install Library, and we are done! The cluster will
now let us import the library from any existing or new notebook. Now, spark_sklearn is
a handy tool that allows the use of some scikit-learn packages with the backend swapped
out for PySpark. This means that we can use existing code that we have already written,
and only have to tweak it slightly to make it compatible with Spark.

Using spark_sklearn to build an MNIST classifier
We have already seen that MNIST is a handwritten digit detection dataset. Without
spending too much time on the data itself, let's jump right into how we can use
spark_sklearn for our benefit. Let's bring in our data using the standard scikit-learn
dataset module:
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV as original_grid_search
digits = datasets.load_digits()
X, y = digits.data, digits.target

From here, we can import our handy GridSearchCV module to do some hyperparameter
tuning:
param_grid = {"max_depth": [3, None],
"max_features": [1, 3, 10],
"min_samples_leaf": [1, 3, 10],
"bootstrap": [True, False],
"criterion": ["gini", "entropy"],
"n_estimators": [10, 100, 1000]}
gs = original_grid_search(RandomForestClassifier(), param_grid=param_grid)
gs.fit(X, y)
gs.best_params_, gs.bestscore
({'bootstrap': False,
'criterion': 'gini',
'max_depth': None,
'max_features': 3,
'min_samples_leaf': 1,
'n_estimators': 1000},
0.95436839176405119)
Command took 24.69 minutes

[ 404 ]

Microsoft Azure Databricks

Chapter 14

The preceding code is a standard grid search that takes nearly 25 minutes to run. As in our
first case study, we could write a custom function to run this in parallel, but that would
take a lot of custom code. As in our second case study, we use could MLlib to write a
scalable grid search using the MLlib standard models, but that would take a while as well.
spark_sklearn provides a third option. We can import their GridSearchCV and swap
out our module for theirs to get extreme gains in speed with minimal code intervention:
# the new gridsearch module
from spark_sklearn import GridSearchCV as spark_grid_search
# the only difference is passing in the SparkContext objecr as the first
parameter of the grid search
gs = spark_grid_search(sc, RandomForestClassifier(), param_grid=param_grid)
gs.fit(X, y)
gs.best_params_, gs.bestscore
({'bootstrap': False,
'criterion': 'gini',
'max_depth': None,
'max_features': 3,
'min_samples_leaf': 1,
'n_estimators': 100},
0.95436839176405119)
Command took 5.29 minute

By doing nothing more than importing a new grid search module and passing the
SparkContext variable into the new module, we can get a 5x speed boost. Always be on
the lookout for third-party modules that can be used to enhance the already easy-to-use
Azure Databricks environment.
More about spark_sklearn can be found on their GitHub page: https:/
/​github.​com/​databricks/​spark-​sklearn.

[ 405 ]

Microsoft Azure Databricks

Chapter 14

Summary
Azure Databricks provides an environment that allows us to create scalable machine
learning pipelines with ease. By using the notebooks in Azure Databricks, we can run our
data analytics and machine learning code in the cloud with powerful Azure resources in
the backend. By implementing MLlib, we can create scalable machine learning pipelines
behind Apache Spark. Utilizing third-party tools is a great way to bring everything together
to build extremely powerful learning algorithms and train them much faster than on our
local machines.

[ 406 ]

Other Books You May Enjoy
If you enjoyed this book, you may be interested in these other books by Packt:

Data Science Algorithms in a Week, Second Edition
Dávid Natingga
ISBN: 978-1-78980-607-6
Understand how to identify a data science problem correctly
Implement well-known machine learning algorithms efficiently using Python
Classify your datasets using Naive Bayes, decision trees, and random forest with
accuracy
Devise an appropriate prediction solution using regression
Work with time series data to identify relevant data events and trends
Cluster your data using the k-means algorithm

Other Books You May Enjoy

Practical Data Science Cookbook, Second Edition
Tony Ojeda, Sean Patrick Murphy, Et al
ISBN: 978-1-78712-962-7
Learn and understand the installation procedure and environment required for R
and Python on various platforms
Prepare data for analysis by implement various data science concepts such as
acquisition, cleaning and munging through R and Python
Build a predictive model and an exploratory model
Analyze the results of your model and create reports on the acquired data
Build various tree-based methods and Build random forest

[ 408 ]

Other Books You May Enjoy

Leave a review - let other readers know what
you think
Please share your thoughts on this book with others by leaving a review on the site that you
bought it from. If you purchased the book from Amazon, please leave us an honest review
on this book's Amazon page. This is vital so that other potential readers can see and use
your unbiased opinion to make purchasing decisions, we can understand what our
customers think about our products, and our authors can see your feedback on the title that
they have worked with Packt to create. It will only take a few minutes of your time, but is
valuable to other potential customers, our authors, and Packt. Thank you!

[ 409 ]

Index
A
Adam Optimizer 374
arithmetic mean 48, 144
arithmetic symbols
about 78
dot product 79, 81
proportional 79
summation 78
AutoRegressive Integrated Moving Average
(ARIMA) 355
average observation 44

B
back-propagation 332
bar charts 193
Bayes' formula 79
Bayes' theorem 112, 113, 115
applications 117
example 117, 119
Bayesian approach
about 97
versus Frequentist approach 97
Bayesian
about 113
bias/variance tradeoff
about 298
error functions 309, 310
errors, due to bias 298
errors, due to variance 298, 299
example 299, 300, 301, 302, 303, 304, 306,
307
overfitting 308
underfitting 308
big data 21
binary classifier 110
binomial random variable 127, 129

Bootstrap aggregation (bagging) 323
box plots 197, 199

C
Cartesian graph 82
causation 201
chi-square goodness of fit test
about 182
assumptions 182
example 183
for association/independence 185
chi-square independence test
assumptions 185
classification 219
classification route 353, 354, 355
classification tree
fitting 266, 268, 270, 271
cluster validation
optimal number, selecting for 282
clustering 221
coefficient of variation 149
collectively exhaustive
about 112
events 112
comma separated value (CSV) 39
communication matter 189
compound events 101
conditional probability 104
confidence intervals 170, 172, 174
confounding factor 142
confusion matrices 110
confusion matrix 110
continuous random variable 133, 136
correlation 201
correlation coefficients 157
correlation
causation, lacking 206

versus causation 201, 203
cross-validation error
versus training error 318, 320

D
data levels
about 42
checking 46
interval level 47
nominal level 43
ordinal level 44
ratio level 51
data pre-processing
example 34
special characters 35
text, relative length 36
topics, picking out 36
word/phrase counts 35
data science, case studies
about 22
dollars, marketing 25
government paper pushing, automating 23
job description 27, 29
performance 24
data science
about 9, 55
computer programming 12, 16
consideration 10
data mining 21
data, exploring 56
data, modeling 57
data, obtaining 56
domain knowledge 12, 20
exploratory data analysis (EDA) 21
machine learning 21
math 13
math/statistics 12
organized data 9
overview 56
Python 16
question 56
results, communicating 57
results, visualizing 57
terminology 9, 21
unorganized data 9

Venn diagram 12, 13
xyz123 Technologies 11
data
about 42, 53
experimental 139
experimentation 139
exploring 57, 58
observational 139
obtaining 139
probability sampling 142
random sampling 142
sampling 139, 141
titanic dataset 68
types 32, 33
unequal probability sampling 143
yelp dataset 59
DataFrame 61
decision trees
about 264, 266
classification tree, fitting 266, 268, 270, 271
comparing, with random forests 329
regression tree, building 266
Deep Neural Network Classifier 368
dimension reduction 221
discrete random variable
about 121, 124, 126
binomial random variables 127, 129
continuous random variable 133, 136
geometric random variable 129
poisson random variable 131
types 127
DNNClassifier 368
domain knowledge 20
dummy variables 248, 249, 250, 251, 252

E
effective visualizations
identifying 189
empirical rule 159, 160
ensembling
about 320, 321, 322
random forests 323, 324, 325, 328
error functions 309, 310
Euler's number 241
event 96

[ 411 ]

exploratory data analysis (EDA) 338, 345, 349,
350
about 339, 340, 341, 343
classification route 353, 354, 355
exploratory data analysis 339
regression route 351
exponent 83, 84

measures of variation 49

J
Jaccard measure 89

K

F
false positive 181
feature extraction 284, 286, 288, 291, 293, 295
Frequentist approach
about 97, 98
law of large num 99
versus Bayesian approach 97

G
geometric mean 51
geometric random variable 130
gradient descent 365
graphs 82, 201
grid searching
about 315, 316, 317, 318
training error, versus cross-validation error 318,
320

H
histograms 195
hypothesis test 175
conducting 176
one sample t-tests 177
hypothesis tests
chi-square goodness of fit test 182
for categorical variables 182
type I errors 181
type II errors 181

I
ineffective visualizations
identifying 189
interval level
about 47
example 47
mathematical operations 48
measures of center 48

k folds cross-validation
about 310, 311, 313, 314
features 312
k-fold cross validation 367
k-means clustering
about 272, 274
beer, illustrative example 279, 281
data points, illustrative example 274, 276, 278
K-Nearest Neighbors (KNN) algorithm 310
K
optimal number, selecting for 282
key performance indicator (KPI) 206

L
labeled data 215
likelihood 256
likert 44
Likert scale 124
line graphs 191
linear algebra 82
about 90
matrix multiplication 90
linear regression 226, 227, 228, 229, 230, 231
predictors, adding 231, 232, 233, 234
regression metrics 234, 235, 237, 239, 240
log odds 242, 243, 244, 245
logarithm 83, 84
logistic regression 240, 241, 242
math 245, 247, 248

M
machine learning
about 15, 211
data 220
facial recognition 211
limitations 213
predictions 218
probabilistic model 21

[ 412 ]

reinforcement learning 225
statistical model 21
supervised learning 215
supervised learning, example 216, 217
supervised learning, types 219
supervised machine learning 224
types 214, 215
types, overview 224
unsupervised learning 221, 222
unsupervised machine learning 225
working 214
math 74
about 14
example 14
mathematics 74, 75
symbols 75
terminology 75
matrices
about 75
answers 78
multiplication 91, 93, 94
matrix 76
matrix multiplication 90
mean 48
Mean Squared Error (MSE) 266
measures of center 43
measures of relative standing 151
correlations, in data 156
measures of variation 49, 151
measures of variation, interval level
standard deviation 49
measures of variation, statistics
defining 149
example 150
median 45, 145
microsoft data science environment
about 375
Microsoft Azure 375
Microsoft Azure Machine Learning Studio 375
Microsoft Databricks 375
Microsoft Databricks
about 376
bike-sharing usage prediction, parallelization in
Databricks 378, 379, 380, 381, 382, 383,
385, 386, 387, 389, 390, 391

cluster, setting up 377
clusters/workers 376
MLib 376
MLlib Grid Search module, used to tune
hyperparameters 399, 402
MLlib, used in Databricks to predict credit card
391, 392, 393, 394, 395, 397, 399
Notebooks 376
Python libraries, adding to cluster 403
reference link 376
Spark DataFrames 376
spark_sklearn, used to build MNIST classifier
404, 405
used, to optimize hyperparameter tuning 403,
404
using 376
model coefficients 229
models 13
multilayer perceptron (MLP) 332
mutually exhaustive 112

N
Naive Bayes
classification 255, 259, 262, 263
Naïve Bayes algorithm 112
neural networks
about 329
basic structure 330
for anomaly detection 330
for entity movement 330
for pattern recognition 330
structure 331, 332, 333, 335, 336
nominal level 60
about 43
data 44
mathematical operations 43
measure of center 43
normalizing constant 256
notation 96
null model 239

O
odds 242, 243, 244, 245
one sample t-tests
about 177

[ 413 ]

assumptions 178, 180
example 178
ordinal level 44, 60
examples 44
mathematical operations 44
measure of center 45
out-of-sample (OOS) error 311
overfitting 308

P
parameter 138
perceptrons 330
point estimates 162, 165, 166
poisson random variable 131
pre-processing 34
prediction 217
predictive analytics models 215
presentation
strategy 208
principal component analysis 284, 286, 288, 291,
293, 295
Principal Component Analysis (PCA) 288
prior probability 256
probability 96, 242, 243, 244, 245
probability density function (PDF) 133
probability mass function (PMF) 122, 127
probability
addition rule 104
complementary events 108
event independence 108
multiplication rule 107
mutual exclusivity 106
rules 104
procedure 95
proper subset 88
PySpark 376
Python
about 16
example 18
practices 17
single tweet, parsing 19

Q
qualitative data
about 37

example 37, 39
exploration tips 62
filtering in pandas 64
nominal level columns 62
ordinal level columns 67
versus quantitative data 36
quantitative data
about 37
continuous data 41
discrete data 41
example 37, 39
overview 41

R
random forests
about 323, 324, 325, 328
comparing, with decision trees 329
random variable 120
discrete random variable 121, 124, 126
random variables 112
ratio level
about 51
example 51
measures of center 51
problems 52
regression 219
regression metrics 234, 235, 237, 239, 240
regression route 351
regression tree
building 266
reinforcement learning 223, 224, 225
relative frequency 98
root-mean-square error (RMSE) 351

S
sample space 96
sampling bias 142
sampling distributions 167, 169
scalar 80
scatter plots 190
set theory 86
Silhouette Coefficient 282, 284
simpson's paradox 204
social media
exploratory data analysis (EDA) 340, 341, 343,

[ 414 ]

345, 349, 350
survey 356, 357, 359, 360, 362, 363
Spark 376
spark_sklearn
reference link 405
square matrix 77
standard deviation 146
standard normal distribution 133
statistical modeling 226
statistics 137, 201
measures of center 144
measures of relative standing 150, 153, 156
measures of variation 145, 148
measuring 144
stock price
example 355
stock prices
predicting, on social media 338
text sentiment analysis 338, 339
structured data
about 33
versus unstructured data 33
superset 87
supervised learning 215
supervised learning models 271
supervised learning
classification 219
regression 219
types 219
supervised machine learning 224

T
TensorFlow
about 368, 369, 370, 371, 372, 374
neural networks 368, 369, 370, 371, 372, 374

using 363, 364, 366, 367
titanic dataset 68
training error
versus cross-validation error 318, 320

U
underfitting 308
unstructured data
about 33
versus structured data 33
unsupervised learning 222
about 221, 271
reinforcement learning 223, 224
using 271
unsupervised machine learning 225

V
vectors
about 75
answers 78
exercises 78
verbal communication
about 206
data findings, presentation 207

W
World Health Organization (WHO) 39

Y
yelp dataset
about 59
DataFrame 61
qualitative data, exploration tips 62
series 62

